#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çŸ¥ä¹æ–‡ç« æŠ“å–å·¥å…· - å•ç¯‡æŠ“å–ç‰ˆ
åªæŠ“å–æ–‡ç« ä¸»ä½“å†…å®¹ï¼Œä¸åŒ…æ‹¬ä¾§è¾¹æ 
"""

import requests
from bs4 import BeautifulSoup
import sys
import os
from datetime import datetime
import time
import json
import re
import random


class ZhihuArticleCrawler:
    def __init__(self, cookie=None):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            cookie: å¯é€‰çš„cookieå­—ç¬¦ä¸²ï¼Œç”¨äºç»•è¿‡ç™»å½•é™åˆ¶
        """
        # å¤šä¸ªUser-Agentè½®æ¢ä½¿ç”¨
        self.user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
        ]
        
        self.base_headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0',
        }
        
        self.cookie = cookie
        self.session = requests.Session()
    
    def _get_headers(self):
        """è·å–è¯·æ±‚å¤´"""
        headers = self.base_headers.copy()
        headers['User-Agent'] = random.choice(self.user_agents)
        headers['Referer'] = 'https://www.zhihu.com/'
        
        if self.cookie:
            headers['Cookie'] = self.cookie
        
        return headers
    
    def _make_request(self, url, max_retries=5):
        """
        å‘é€HTTPè¯·æ±‚ï¼Œå¸¦æ™ºèƒ½é‡è¯•
        
        Args:
            url: ç›®æ ‡URL
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            requests.Response: å“åº”å¯¹è±¡
        """
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    # æŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨
                    wait_time = (2 ** attempt) + random.uniform(0, 1)
                    print(f"âš ï¸  ç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                
                headers = self._get_headers()
                
                response = self.session.get(
                    url,
                    headers=headers,
                    timeout=20,
                    allow_redirects=True
                )
                
                # æ£€æŸ¥æ˜¯å¦è¢«åçˆ¬è™«æ‹¦æˆª
                if response.status_code == 403:
                    if attempt < max_retries - 1:
                        print(f"âš ï¸  é‡åˆ°403é”™è¯¯ï¼Œå°è¯•æ›´æ¢User-Agenté‡è¯• ({attempt + 1}/{max_retries})")
                        continue
                    else:
                        raise Exception("403 Forbidden - éœ€è¦æä¾›æœ‰æ•ˆçš„Cookie")
                
                if response.status_code == 429:
                    wait_time = 5 + random.uniform(0, 5)
                    print(f"âš ï¸  è¯·æ±‚è¿‡å¿«ï¼Œç­‰å¾… {wait_time:.1f} ç§’...")
                    time.sleep(wait_time)
                    continue
                
                response.raise_for_status()
                response.encoding = 'utf-8'
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯éªŒè¯é¡µé¢
                if 'zh-zse-ck' in response.text or len(response.text) < 1000:
                    if attempt < max_retries - 1:
                        print(f"âš ï¸  æ£€æµ‹åˆ°éªŒè¯é¡µé¢ï¼Œé‡è¯•ä¸­ ({attempt + 1}/{max_retries})")
                        time.sleep(3)
                        continue
                    else:
                        raise Exception("é‡åˆ°éªŒè¯é¡µé¢ï¼Œè¯·æä¾›æœ‰æ•ˆçš„Cookie")
                
                return response
                
            except requests.Timeout:
                if attempt < max_retries - 1:
                    print(f"âš ï¸  è¯·æ±‚è¶…æ—¶ï¼Œé‡è¯•ä¸­ ({attempt + 1}/{max_retries})")
                    continue
                else:
                    raise Exception("è¯·æ±‚è¶…æ—¶")
            
            except requests.RequestException as e:
                if attempt < max_retries - 1:
                    print(f"âš ï¸  è¯·æ±‚å¤±è´¥: {str(e)}, é‡è¯•ä¸­ ({attempt + 1}/{max_retries})")
                    continue
                else:
                    raise Exception(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
        
        raise Exception("è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
    
    def fetch_article(self, url):
        """
        æŠ“å–çŸ¥ä¹æ–‡ç« 
        
        Args:
            url: çŸ¥ä¹æ–‡ç« URL
            
        Returns:
            tuple: (title, content_html, author, publish_time) æ–‡ç« ä¿¡æ¯
        """
        response = self._make_request(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æå–æ–‡ç« æ ‡é¢˜ - å¤šç§æ–¹å¼å°è¯•
        title = None
        title_selectors = [
            ('h1', {'class': 'Post-Title'}),
            ('h1', {'class': 'ArticleTitle'}),
            ('h1', {}),
            ('title', {})
        ]
        
        for tag, attrs in title_selectors:
            title = soup.find(tag, attrs)
            if title:
                break
        
        title_text = title.get_text().strip() if title else 'æœªçŸ¥æ ‡é¢˜'
        # æ¸…ç†æ ‡é¢˜ä¸­çš„ç‰¹æ®Šå­—ç¬¦
        title_text = re.sub(r'\s+', ' ', title_text)
        
        # æå–æ–‡ç« ä¸»ä½“å†…å®¹ - å¤šç§é€‰æ‹©å™¨
        article_content = None
        content_selectors = [
            ('div', {'class': 'Post-RichTextContainer'}),
            ('div', {'class': 'RichText'}),
            ('div', {'class': 'Post-RichText'}),
            ('article', {'class': 'Post-Main'}),
            ('article', {}),
            ('div', {'class': 'content'}),
        ]
        
        for tag, attrs in content_selectors:
            article_content = soup.find(tag, attrs)
            if article_content and len(article_content.get_text().strip()) > 100:
                break
        
        if not article_content:
            raise Exception("æœªæ‰¾åˆ°æ–‡ç« å†…å®¹ï¼Œå¯èƒ½éœ€è¦ç™»å½•æˆ–Cookie")
        
        # æå–ä½œè€…ä¿¡æ¯
        author_name = ''
        author_selectors = [
            ('div', {'class': 'AuthorInfo'}),
            ('div', {'class': 'author-info'}),
            ('a', {'class': 'UserLink'}),
        ]
        
        for tag, attrs in author_selectors:
            author_info = soup.find(tag, attrs)
            if author_info:
                author_link = author_info.find('a', class_='UserLink')
                if not author_link:
                    author_link = author_info.find('a')
                if author_link:
                    author_name = author_link.get_text().strip()
                    break
        
        # æå–å‘å¸ƒæ—¶é—´
        publish_time_text = ''
        time_selectors = [
            ('div', {'class': 'ContentItem-time'}),
            ('div', {'class': 'publish-time'}),
            ('time', {}),
        ]
        
        for tag, attrs in time_selectors:
            publish_time = soup.find(tag, attrs)
            if publish_time:
                publish_time_text = publish_time.get_text().strip()
                break
        
        return title_text, article_content, author_name, publish_time_text
    
    def save_to_html(self, url, output_file=None):
        """
        æŠ“å–æ–‡ç« å¹¶ä¿å­˜ä¸ºHTMLæ–‡ä»¶
        
        Args:
            url: çŸ¥ä¹æ–‡ç« URL
            output_file: è¾“å‡ºæ–‡ä»¶åï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            
        Returns:
            str: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        print(f"æ­£åœ¨æŠ“å–: {url}")
        
        title, content, author, publish_time = self.fetch_article(url)
        
        print(f"âœ“ æ–‡ç« æ ‡é¢˜: {title}")
        if author:
            print(f"âœ“ ä½œè€…: {author}")
        if publish_time:
            print(f"âœ“ å‘å¸ƒæ—¶é—´: {publish_time}")
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        if not output_file:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            safe_title = "".join(c for c in title if c.isalnum() or c in (' ', '-', '_')).strip()
            safe_title = safe_title[:50]  # é™åˆ¶æ–‡ä»¶åé•¿åº¦
            output_file = f"{safe_title}_{timestamp}.html"
        
        # æ„å»ºå®Œæ•´çš„HTMLæ–‡æ¡£
        html_template = f"""<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{title}</title>
    <style>
        body {{
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: #333;
        }}
        h1 {{
            font-size: 28px;
            font-weight: 600;
            margin-bottom: 20px;
            color: #1a1a1a;
        }}
        .meta-info {{
            color: #8590a6;
            font-size: 14px;
            margin-bottom: 20px;
            padding-bottom: 20px;
            border-bottom: 1px solid #e5e5e5;
        }}
        .article-content {{
            font-size: 16px;
            color: #1a1a1a;
        }}
        .article-content img {{
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px 0;
        }}
        .article-content p {{
            margin: 15px 0;
        }}
        .article-content pre {{
            background: #f6f6f6;
            padding: 15px;
            border-radius: 4px;
            overflow-x: auto;
        }}
        .article-content code {{
            background: #f6f6f6;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: "Courier New", monospace;
        }}
        .article-content blockquote {{
            border-left: 3px solid #ddd;
            padding-left: 15px;
            color: #666;
            margin: 15px 0;
        }}
    </style>
</head>
<body>
    <h1>{title}</h1>
    <div class="meta-info">
        {f'<div>ä½œè€…: {author}</div>' if author else ''}
        {f'<div>å‘å¸ƒæ—¶é—´: {publish_time}</div>' if publish_time else ''}
        <div>åŸæ–‡é“¾æ¥: <a href="{url}">{url}</a></div>
    </div>
    <div class="article-content">
        {content}
    </div>
</body>
</html>"""
        
        # ä¿å­˜æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html_template)
        
        print(f"âœ“ æ–‡ç« å·²ä¿å­˜åˆ°: {output_file}")
        return output_file


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("=" * 70)
        print("çŸ¥ä¹æ–‡ç« æŠ“å–å·¥å…· - å•ç¯‡æŠ“å–ç‰ˆ")
        print("=" * 70)
        print("\nä½¿ç”¨æ–¹æ³•:")
        print("  python3 zhihu_crawler.py <URL> [è¾“å‡ºæ–‡ä»¶] [--cookie COOKIE]")
        print("\nç¤ºä¾‹:")
        print("  # åŸºç¡€æŠ“å–")
        print("  python3 zhihu_crawler.py https://zhuanlan.zhihu.com/p/123456")
        print("\n  # æŒ‡å®šè¾“å‡ºæ–‡ä»¶")
        print("  python3 zhihu_crawler.py https://zhuanlan.zhihu.com/p/123456 output.html")
        print("\n  # ä½¿ç”¨Cookieï¼ˆæ¨èï¼‰")
        print("  python3 zhihu_crawler.py https://zhuanlan.zhihu.com/p/123456 --cookie 'xxx'")
        print("\nè·å–Cookieçš„æ–¹æ³•:")
        print("  1. åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€çŸ¥ä¹å¹¶ç™»å½•")
        print("  2. æŒ‰F12æ‰“å¼€å¼€å‘è€…å·¥å…·ï¼Œåˆ‡æ¢åˆ°Networkæ ‡ç­¾")
        print("  3. åˆ·æ–°é¡µé¢ï¼Œæ‰¾åˆ°æ–‡ç« è¯·æ±‚ï¼Œå¤åˆ¶Cookieå€¼")
        print("  4. æˆ–è¿è¡Œ: python3 get_cookie_helper.py")
        print("=" * 70)
        sys.exit(1)
    
    # è§£æå‚æ•°
    url = None
    output_file = None
    cookie = None
    
    i = 1
    while i < len(sys.argv):
        arg = sys.argv[i]
        
        if arg == '--cookie':
            if i + 1 < len(sys.argv):
                cookie = sys.argv[i + 1]
                i += 2
            else:
                print("âœ— é”™è¯¯: --cookie éœ€è¦æä¾›Cookieå€¼")
                sys.exit(1)
        elif arg.startswith('--'):
            print(f"âœ— é”™è¯¯: æœªçŸ¥é€‰é¡¹ {arg}")
            sys.exit(1)
        else:
            if not url:
                url = arg
            elif not output_file:
                output_file = arg
            i += 1
    
    if not url:
        print("âœ— é”™è¯¯: è¯·æä¾›URL")
        sys.exit(1)
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = ZhihuArticleCrawler(cookie=cookie)
    
    try:
        crawler.save_to_html(url, output_file)
        print("\nâœ… æŠ“å–æˆåŠŸï¼")
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        sys.exit(130)
    except Exception as e:
        print(f"\nâœ— é”™è¯¯: {str(e)}")
        if '403' in str(e) or 'Cookie' in str(e):
            print("\nğŸ’¡ æç¤º: éœ€è¦æä¾›æœ‰æ•ˆçš„Cookie")
            print("   è¿è¡Œ 'python3 get_cookie_helper.py' æŸ¥çœ‹å¦‚ä½•è·å–Cookie")
        sys.exit(1)


if __name__ == '__main__':
    main()
