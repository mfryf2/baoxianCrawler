#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çŸ¥ä¹æ–‡ç« æŠ“å–å·¥å…· - å•ç¯‡æŠ“å–ç‰ˆ
åªæŠ“å–æ–‡ç« ä¸»ä½“å†…å®¹ï¼Œä¸åŒ…æ‹¬ä¾§è¾¹æ 
"""

import requests
from bs4 import BeautifulSoup
import sys
import os
from datetime import datetime
import time
import json
import re
import random
import pymysql
from pymysql.cursors import DictCursor


class ZhihuArticleCrawler:
    def __init__(self, cookie=None, db_config=None):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            cookie: å¯é€‰çš„cookieå­—ç¬¦ä¸²ï¼Œç”¨äºç»•è¿‡ç™»å½•é™åˆ¶
            db_config: æ•°æ®åº“é…ç½®å­—å…¸ (host, user, password, database, port)
        """
        # å¤šä¸ªUser-Agentè½®æ¢ä½¿ç”¨
        self.user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
        ]
        
        self.base_headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
        }
        
        self.cookie = cookie
        self.session = requests.Session()
        self.db_config = db_config
    
    def _get_headers(self):
        """è·å–è¯·æ±‚å¤´"""
        headers = self.base_headers.copy()
        headers['User-Agent'] = random.choice(self.user_agents)
        headers['Referer'] = 'https://www.zhihu.com/'
        
        if self.cookie:
            headers['Cookie'] = self.cookie
        
        return headers
    
    def _make_request(self, url, max_retries=5):
        """
        å‘é€HTTPè¯·æ±‚ï¼Œå¸¦æ™ºèƒ½é‡è¯•
        
        Args:
            url: ç›®æ ‡URL
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            requests.Response: å“åº”å¯¹è±¡
        """
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    # æŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨
                    wait_time = (2 ** attempt) + random.uniform(0, 1)
                    print(f"âš ï¸  ç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                
                headers = self._get_headers()
                
                response = self.session.get(
                    url,
                    headers=headers,
                    timeout=20,
                    allow_redirects=True
                )
                
                # æ£€æŸ¥æ˜¯å¦è¢«åçˆ¬è™«æ‹¦æˆª
                if response.status_code == 403:
                    if attempt < max_retries - 1:
                        print(f"âš ï¸  é‡åˆ°403é”™è¯¯ï¼Œå°è¯•æ›´æ¢User-Agenté‡è¯• ({attempt + 1}/{max_retries})")
                        continue
                    else:
                        raise Exception("403 Forbidden - éœ€è¦æä¾›æœ‰æ•ˆçš„Cookie")
                
                if response.status_code == 429:
                    wait_time = 5 + random.uniform(0, 5)
                    print(f"âš ï¸  è¯·æ±‚è¿‡å¿«ï¼Œç­‰å¾… {wait_time:.1f} ç§’...")
                    time.sleep(wait_time)
                    continue
                
                response.raise_for_status()
                response.encoding = 'utf-8'
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯éªŒè¯é¡µé¢
                if 'zh-zse-ck' in response.text or len(response.text) < 1000:
                    if attempt < max_retries - 1:
                        print(f"âš ï¸  æ£€æµ‹åˆ°éªŒè¯é¡µé¢ï¼Œé‡è¯•ä¸­ ({attempt + 1}/{max_retries})")
                        time.sleep(3)
                        continue
                    else:
                        raise Exception("é‡åˆ°éªŒè¯é¡µé¢ï¼Œè¯·æä¾›æœ‰æ•ˆçš„Cookie")
                
                return response
                
            except requests.Timeout:
                if attempt < max_retries - 1:
                    print(f"âš ï¸  è¯·æ±‚è¶…æ—¶ï¼Œé‡è¯•ä¸­ ({attempt + 1}/{max_retries})")
                    continue
                else:
                    raise Exception("è¯·æ±‚è¶…æ—¶")
            
            except requests.RequestException as e:
                if attempt < max_retries - 1:
                    print(f"âš ï¸  è¯·æ±‚å¤±è´¥: {str(e)}, é‡è¯•ä¸­ ({attempt + 1}/{max_retries})")
                    continue
                else:
                    raise Exception(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
        
        raise Exception("è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
    
    def _clean_html_content(self, html_element):
        """
        æ¸…ç†HTMLå†…å®¹ï¼Œç§»é™¤æ— æ•ˆçš„CSSå’Œå†—ä½™æ ·å¼
        
        Args:
            html_element: BeautifulSoupå…ƒç´ 
            
        Returns:
            str: æ¸…ç†åçš„HTMLå­—ç¬¦ä¸²
        """
        # ç§»é™¤æ‰€æœ‰styleæ ‡ç­¾ï¼ˆç‰¹åˆ«æ˜¯emotion-cssæ ‡ç­¾ï¼‰
        for style_tag in html_element.find_all('style'):
            style_tag.decompose()
        
        # ç§»é™¤æ‰€æœ‰data-emotion-csså±æ€§å’Œå…¶ä»–æ— ç”¨å±æ€§
        for element in html_element.find_all(True):  # Trueè¡¨ç¤ºæ‰€æœ‰å…ƒç´ 
            # ç§»é™¤æ— ç”¨çš„å±æ€§
            attrs_to_remove = [
                'data-emotion-css',
                'class',  # ç§»é™¤emotionç”Ÿæˆçš„ç±»å
                'data-pid',
                'data-draft-type',
                'data-first-child',
                'data-search-entity',
                'data-caption',
                'data-original',
                'data-original-token',
                'data-rawheight',
                'data-rawwidth',
                'data-size',
                'eeimg',
            ]
            
            for attr in attrs_to_remove:
                if element.has_attr(attr):
                    del element[attr]
        
        # è¿”å›æ¸…ç†åçš„HTMLå­—ç¬¦ä¸²
        return str(html_element)
    
    def _extract_like_count(self, soup):
        """
        æå–æ–‡ç« çš„èµåŒæ•°ï¼ˆç‚¹èµæ•°ï¼‰
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            int: èµåŒæ•°ï¼Œå¦‚æœæ— æ³•æå–åˆ™è¿”å›-1
        """
        try:
            # å°è¯•å¤šç§æ–¹å¼æå–èµåŒæ•°
            selectors = [
                {'tag': 'button', 'attrs': {'aria-label': re.compile(r'.*èµåŒ.*')}},
                {'tag': 'div', 'attrs': {'class': re.compile(r'.*like.*|.*VoteButton.*')}},
                {'tag': 'span', 'attrs': {'class': re.compile(r'.*count.*|.*number.*')}},
            ]
            
            for selector in selectors:
                elements = soup.find_all(selector['tag'], selector['attrs'])
                for element in elements:
                    text = element.get_text().strip()
                    # æå–æ•°å­—
                    numbers = re.findall(r'\d+', text)
                    if numbers:
                        return int(numbers[0])
            
            # å°è¯•ä»é¡µé¢æºç ä¸­æå–
            page_text = soup.get_text()
            match = re.search(r'èµåŒ\s*(\d+)', page_text)
            if match:
                return int(match.group(1))
                
        except Exception as e:
            print(f"âš ï¸  æå–èµåŒæ•°å¤±è´¥: {str(e)}")
        
        return -1
    
    def fetch_article(self, url):
        """
        æŠ“å–çŸ¥ä¹æ–‡ç« 
        
        Args:
            url: çŸ¥ä¹æ–‡ç« URL
            
        Returns:
            dict: æ–‡ç« ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å« title, content, author, publish_time, like_count
        """
        response = self._make_request(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æå–æ–‡ç« æ ‡é¢˜ - å¤šç§æ–¹å¼å°è¯•
        title = None
        title_selectors = [
            ('h1', {'class': 'Post-Title'}),
            ('h1', {'class': 'ArticleTitle'}),
            ('h1', {}),
            ('title', {})
        ]
        
        for tag, attrs in title_selectors:
            title = soup.find(tag, attrs)
            if title:
                break
        
        title_text = title.get_text().strip() if title else 'æœªçŸ¥æ ‡é¢˜'
        # æ¸…ç†æ ‡é¢˜ä¸­çš„ç‰¹æ®Šå­—ç¬¦
        title_text = re.sub(r'\s+', ' ', title_text)
        
        # æå–æ–‡ç« ä¸»ä½“å†…å®¹ - å¤šç§é€‰æ‹©å™¨
        article_content = None
        content_selectors = [
            ('div', {'class': 'Post-RichTextContainer'}),
            ('div', {'class': 'RichText'}),
            ('div', {'class': 'Post-RichText'}),
            ('article', {'class': 'Post-Main'}),
            ('article', {}),
            ('div', {'class': 'content'}),
        ]
        
        for tag, attrs in content_selectors:
            found_element = soup.find(tag, attrs)
            if found_element:
                content_text = found_element.get_text().strip()
                if len(content_text) > 100:
                    article_content = found_element
                    break
        
        if not article_content:
            raise Exception("æœªæ‰¾åˆ°æ–‡ç« å†…å®¹ï¼Œå¯èƒ½éœ€è¦ç™»å½•æˆ–Cookie")
        
        # æ¸…ç†å†…å®¹ä¸­çš„CSSå’Œæ— ç”¨å±æ€§
        article_content = BeautifulSoup(self._clean_html_content(article_content), 'html.parser')
        
        # æå–ä½œè€…ä¿¡æ¯ - æ”¹è¿›çš„å¤šç§æ–¹å¼
        author_name = ''
        
        # ç­–ç•¥ 1: ä» Meta itemprop æ ‡ç­¾æå–
        if not author_name:
            try:
                author_meta = soup.find('meta', {'itemprop': 'name'})
                if author_meta and author_meta.get('content'):
                    author_name = author_meta['content'].strip()
            except:
                pass
        
        # ç­–ç•¥ 2: ä» UserLink-link ç±»çš„é“¾æ¥ä¸­æå–
        if not author_name:
            try:
                author_link = soup.find('a', {'class': 'UserLink-link'})
                if author_link:
                    author_name = author_link.get_text().strip()
            except:
                pass
        
        # ç­–ç•¥ 3: ä» Post-Author åŒºåŸŸä¸­æŸ¥æ‰¾
        if not author_name:
            try:
                post_author = soup.find('div', {'class': 'Post-Author'})
                if post_author:
                    # å°è¯•ä»é“¾æ¥ä¸­æå–
                    author_link = post_author.find('a', {'class': 'UserLink-link'})
                    if author_link:
                        author_name = author_link.get_text().strip()
                    
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä» Meta æ ‡ç­¾æå–
                    if not author_name:
                        meta_name = post_author.find('meta', {'itemprop': 'name'})
                        if meta_name and meta_name.get('content'):
                            author_name = meta_name['content'].strip()
            except:
                pass
        
        # ç­–ç•¥ 4: ä» AuthorInfo ä¸­æå–
        if not author_name:
            author_selectors = [
                ('div', {'class': 'AuthorInfo'}),
                ('div', {'class': 'author-info'}),
                ('span', {'class': re.compile(r'.*author.*', re.I)}),
            ]
            
            for tag, attrs in author_selectors:
                author_info = soup.find(tag, attrs)
                if author_info:
                    # å°è¯•ä»é“¾æ¥ä¸­æå–
                    author_link = author_info.find('a', {'class': 'UserLink-link'})
                    if not author_link:
                        author_link = author_info.find('a')
                    if author_link:
                        author_name = author_link.get_text().strip()
                        if author_name:
                            break
        
        # æå–å‘å¸ƒæ—¶é—´ - æ”¹è¿›çš„å¤šç§æ–¹å¼
        publish_time_text = ''
        time_selectors = [
            ('div', {'class': 'ContentItem-time'}),
            ('div', {'class': 'publish-time'}),
            ('time', {}),
            ('meta', {'property': 'article:published_time'}),  # æ–°å¢ï¼šä» meta æ ‡ç­¾
            ('span', {'class': re.compile(r'.*time.*|.*date.*', re.I)}),  # æ–°å¢ï¼šæ­£åˆ™åŒ¹é…
        ]
        
        for tag, attrs in time_selectors:
            if tag == 'meta':
                # meta æ ‡ç­¾ä½¿ç”¨ content å±æ€§
                time_elem = soup.find(tag, attrs)
                if time_elem and time_elem.get('content'):
                    publish_time_text = time_elem['content'].strip()
                    break
            else:
                publish_time = soup.find(tag, attrs)
                if publish_time:
                    publish_time_text = publish_time.get_text().strip()
                    break
        
        # æå–èµåŒæ•°
        like_count = self._extract_like_count(soup)
        
        return {
            'title': title_text,
            'content': article_content,
            'author': author_name,
            'publish_time': publish_time_text,
            'like_count': like_count
        }
    
    def save_to_html(self, url, output_file=None):
        """
        æŠ“å–æ–‡ç« å¹¶ä¿å­˜ä¸ºHTMLæ–‡ä»¶
        
        Args:
            url: çŸ¥ä¹æ–‡ç« URL
            output_file: è¾“å‡ºæ–‡ä»¶åï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            
        Returns:
            str: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        print(f"æ­£åœ¨æŠ“å–: {url}")
        
        article_info = self.fetch_article(url)
        title = article_info['title']
        content = article_info['content']
        author = article_info['author']
        publish_time = article_info['publish_time']
        
        print(f"âœ“ æ–‡ç« æ ‡é¢˜: {title}")
        if author:
            print(f"âœ“ ä½œè€…: {author}")
        if publish_time:
            print(f"âœ“ å‘å¸ƒæ—¶é—´: {publish_time}")
        if article_info['like_count'] >= 0:
            print(f"âœ“ èµåŒæ•°: {article_info['like_count']}")
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        if not output_file:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            safe_title = "".join(c for c in title if c.isalnum() or c in (' ', '-', '_')).strip()
            safe_title = safe_title[:50]  # é™åˆ¶æ–‡ä»¶åé•¿åº¦
            output_file = f"{safe_title}_{timestamp}.html"
        
        # æ„å»ºå®Œæ•´çš„HTMLæ–‡æ¡£
        html_template = f"""<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{title}</title>
    <style>
        body {{
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: #333;
        }}
        h1, h2, h3, h4, h5, h6 {{
            font-weight: 600;
            color: #1a1a1a;
            margin-top: 1.5em;
            margin-bottom: 0.8em;
        }}
        h1 {{
            font-size: 28px;
        }}
        h2 {{
            font-size: 24px;
        }}
        h3 {{
            font-size: 20px;
        }}
        .meta-info {{
            color: #8590a6;
            font-size: 14px;
            margin-bottom: 20px;
            padding-bottom: 20px;
            border-bottom: 1px solid #e5e5e5;
        }}
        .article-content {{
            font-size: 16px;
            color: #1a1a1a;
            word-break: break-word;
        }}
        .article-content img {{
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px 0;
        }}
        .article-content p {{
            margin: 15px 0;
        }}
        .article-content ul, .article-content ol {{
            margin: 15px 0;
            padding-left: 2em;
        }}
        .article-content li {{
            margin: 8px 0;
        }}
        .article-content pre {{
            background: #f6f6f6;
            padding: 15px;
            border-radius: 4px;
            overflow-x: auto;
            font-size: 14px;
        }}
        .article-content code {{
            background: #f6f6f6;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: "Courier New", monospace;
            font-size: 0.95em;
        }}
        .article-content pre code {{
            background: none;
            padding: 0;
        }}
        .article-content blockquote {{
            border-left: 3px solid #ddd;
            padding-left: 15px;
            color: #666;
            margin: 15px 0;
        }}
        .article-content table {{
            border-collapse: collapse;
            width: 100%;
            margin: 20px 0;
        }}
        .article-content table td, .article-content table th {{
            border: 1px solid #ddd;
            padding: 10px;
        }}
        .article-content table th {{
            background: #f6f6f6;
            font-weight: 600;
        }}
        .article-content hr {{
            margin: 30px 0;
            border: none;
            border-top: 1px solid #ddd;
        }}
        a {{
            color: #09408e;
            text-decoration: none;
            border-bottom: 1px solid #81858f;
        }}
        a:hover {{
            border-bottom-color: #09408e;
        }}
    </style>
</head>
<body>
    <h1>{title}</h1>
    <div class="meta-info">
        {f'<div>ä½œè€…: {author}</div>' if author else ''}
        {f'<div>å‘å¸ƒæ—¶é—´: {publish_time}</div>' if publish_time else ''}
        {f'<div>èµåŒæ•°: {article_info["like_count"]}</div>' if article_info['like_count'] >= 0 else ''}
        <div>åŸæ–‡é“¾æ¥: <a href="{url}">{url}</a></div>
    </div>
    <div class="article-content">
        {content}
    </div>
</body>
</html>"""
        
        # ä¿å­˜æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html_template)
        
        print(f"âœ“ æ–‡ç« å·²ä¿å­˜åˆ°: {output_file}")
        return output_file
    
    def _parse_publish_time(self, time_text):
        """
        è§£æå‘å¸ƒæ—¶é—´ï¼Œæ”¯æŒå¤šç§æ ¼å¼
        
        Args:
            time_text: åŸå§‹æ—¶é—´æ–‡æœ¬
            
        Returns:
            datetime: è§£æåçš„æ—¶é—´å¯¹è±¡ï¼Œæˆ–None
        """
        if not time_text:
            return None
        
        # æ¸…ç†æ–‡æœ¬
        time_text = time_text.strip()
        
        # å°è¯•å¤šç§è§£ææ–¹å¼
        patterns = [
            # ISO æ ¼å¼: 2024-11-03T12:34:56
            (r'(\d{4})-(\d{2})-(\d{2})T(\d{2}):(\d{2}):(\d{2})', '%Y-%m-%dT%H:%M:%S'),
            # æ ‡å‡†æ ¼å¼: 2024-11-03 12:34:56 æˆ– 2024-11-03 12:34
            (r'(\d{4})-(\d{2})-(\d{2})\s+(\d{2}):(\d{2})(?::(\d{2}))?', None),  # å¤„ç†å¯é€‰ç§’æ•°
            # çŸ¥ä¹æ ¼å¼: å‘å¸ƒäº 2024-12-27 10:59ãƒ»å¹¿ä¸œ (æ— æ³•è§£æå…·ä½“æ—¶é—´)
            (r'å‘å¸ƒäº\s+(\d{4})-(\d{2})-(\d{2})\s+(\d{2}):(\d{2})', '%Y-%m-%d %H:%M'),
            # ä¸­æ–‡æ—¥æœŸæ ¼å¼: 2024å¹´12æœˆ27æ—¥ 10:59
            (r'(\d{4})å¹´(\d{2})æœˆ(\d{2})æ—¥\s+(\d{2}):(\d{2})', None),  # éœ€è¦ç‰¹æ®Šå¤„ç†
            # ç›¸å¯¹æ—¶é—´: æŸæ—¶é—´å‰ (è¿™ç§æƒ…å†µæ— æ³•ç²¾å‡†è§£æ)
        ]
        
        for pattern, format_str in patterns:
            match = re.search(pattern, time_text)
            if match:
                try:
                    if format_str:
                        # ç›´æ¥ä½¿ç”¨æŒ‡å®šçš„æ ¼å¼
                        return datetime.strptime(match.group(0), format_str)
                    else:
                        # éœ€è¦ç‰¹æ®Šå¤„ç†çš„æ ¼å¼
                        if 'å¹´' in time_text:
                            # ä¸­æ–‡æ—¥æœŸæ ¼å¼
                            year, month, day, hour, minute = match.groups()
                            return datetime(int(year), int(month), int(day), int(hour), int(minute))
                        else:
                            # æ ‡å‡†æ ¼å¼ä½†å¯èƒ½æ²¡æœ‰ç§’æ•°
                            groups = match.groups()
                            year, month, day, hour, minute = groups[:5]
                            second = int(groups[5]) if groups[5] else 0
                            return datetime(int(year), int(month), int(day), int(hour), int(minute), second)
                except Exception as e:
                    print(f"âš ï¸  è§£ææ—¶é—´æ ¼å¼å¤±è´¥: {str(e)}")
                    continue
        
        return None
    
    def _get_db_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        if not self.db_config:
            raise Exception("æœªé…ç½®æ•°æ®åº“ä¿¡æ¯")
        
        return pymysql.connect(
            host=self.db_config['host'],
            user=self.db_config['user'],
            password=self.db_config['password'],
            database=self.db_config['database'],
            port=self.db_config.get('port', 3306),
            charset='utf8mb4'
        )
    
    def fetch_urls_from_db(self, limit=10):
        """
        ä»æ•°æ®åº“è·å–æ–‡ç« URLåˆ—è¡¨
        
        Args:
            limit: è·å–çš„æ–‡ç« æ•°é‡
            
        Returns:
            list: URLåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« (id, src_url)
        """
        try:
            connection = self._get_db_connection()
            cursor = connection.cursor(DictCursor)
            
            # æŸ¥è¯¢æœªçˆ¬å–çš„æ–‡ç« ï¼ˆsrc_urlä¸ä¸ºç©ºï¼Œä¸”è¿˜æœªæœ‰src_contentçš„ï¼‰
            sql = "SELECT id, src_url FROM baoxianblog WHERE src_url IS NOT NULL AND src_url LIKE %s AND (src_content IS NULL OR src_content = '') LIMIT %s"
            
            cursor.execute(sql, ('%zhihu%', limit))
            results = cursor.fetchall()
            
            cursor.close()
            connection.close()
            
            print(f"âœ“ ä»æ•°æ®åº“è·å–äº† {len(results)} ç¯‡æ–‡ç« URL")
            return results
            
        except Exception as e:
            print(f"âœ— æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {str(e)}")
            raise
    
    def save_article_to_db(self, article_id, url, article_info):
        """
        å°†æŠ“å–çš„æ–‡ç« ä¿¡æ¯ä¿å­˜åˆ°æ•°æ®åº“
        
        Args:
            article_id: æ•°æ®åº“ä¸­çš„æ–‡ç« ID
            url: åŸå§‹URL
            article_info: æŠ“å–çš„æ–‡ç« ä¿¡æ¯å­—å…¸
            
        Returns:
            bool: æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        try:
            connection = self._get_db_connection()
            cursor = connection.cursor()
            
            # è·å–HTMLå†…å®¹
            content_html = str(article_info['content'])
            content_size = len(content_html)
            
            # è§£æå‘å¸ƒæ—¶é—´ - ä½¿ç”¨æ”¹è¿›çš„æ—¶é—´è§£æå™¨
            publish_time = None
            original_time_text = article_info['publish_time']
            
            if article_info['publish_time']:
                publish_time = self._parse_publish_time(article_info['publish_time'])
            
            # å‡†å¤‡æ›´æ–°æ•°æ®
            update_query = """
            UPDATE baoxianblog 
            SET 
                src_title = %s,
                src_content = %s,
                dst_title = %s,
                dst_content = %s,
                src_user = %s,
                like_count = %s,
                src_published_time = %s,
                update_time = NOW(),
                from_source = 'zhihu'
            WHERE id = %s
            """
            
            cursor.execute(update_query, (
                article_info['title'],           # src_title - æ¥æºæ ‡é¢˜
                content_html,                     # src_content - æ¥æºå†…å®¹ï¼ˆåŸæ–‡æ­£æ–‡HTMLï¼‰
                article_info['title'],           # dst_title - ç›®æ ‡æ ‡é¢˜
                content_html,                     # dst_content - ç›®æ ‡å†…å®¹
                article_info['author'],           # src_user - åŸæ–‡ä½œè€…
                article_info['like_count'],       # like_count - èµåŒæ•°
                publish_time,                     # src_published_time - åŸæ–‡å‘å¸ƒæ—¶é—´
                article_id                        # id
            ))
            
            connection.commit()
            cursor.close()
            connection.close()
            
            # è¾“å‡ºè¯¦ç»†çš„ä¿å­˜ä¿¡æ¯
            print(f"âœ“ æ–‡ç« å·²ä¿å­˜åˆ°æ•°æ®åº“ (ID: {article_id})")
            print(f"  â”œâ”€ æ ‡é¢˜: {article_info['title'][:60]}{'...' if len(article_info['title']) > 60 else ''}")
            print(f"  â”œâ”€ ä½œè€…: {article_info['author'] or 'ï¼ˆæœªæ‰¾åˆ°ï¼‰'}")
            print(f"  â”œâ”€ èµåŒæ•°: {article_info['like_count']}")
            
            if original_time_text:
                time_display = original_time_text[:50]  # åªæ˜¾ç¤ºå‰50ä¸ªå­—ç¬¦
                if publish_time:
                    time_display += f" â†’ {publish_time.strftime('%Y-%m-%d %H:%M:%S')}"
                else:
                    time_display += " (æ— æ³•è§£æå…·ä½“æ—¶é—´)"
                print(f"  â”œâ”€ å‘å¸ƒæ—¶é—´: {time_display}")
            else:
                print(f"  â”œâ”€ å‘å¸ƒæ—¶é—´: ï¼ˆæœªæ‰¾åˆ°ï¼‰")
            
            print(f"  â””â”€ å†…å®¹å¤§å°: {content_size} å­—ç¬¦")
            
            return True
            
        except Exception as e:
            print(f"âœ— ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥: {str(e)}")
            return False
    
    def batch_crawl_and_save(self, limit=10):
        """
        æ‰¹é‡æŠ“å–æ–‡ç« å¹¶ä¿å­˜åˆ°æ•°æ®åº“
        
        Args:
            limit: æŠ“å–çš„æ–‡ç« æ•°é‡
        """
        print(f"\n{'='*70}")
        print(f"å¼€å§‹æ‰¹é‡æŠ“å–çŸ¥ä¹æ–‡ç«  (é™åˆ¶: {limit} ç¯‡)")
        print(f"{'='*70}\n")
        
        # ä»æ•°æ®åº“è·å–URLåˆ—è¡¨
        articles = self.fetch_urls_from_db(limit)
        
        if not articles:
            print("âœ— æ²¡æœ‰å¯æŠ“å–çš„æ–‡ç« ")
            return
        
        success_count = 0
        failed_count = 0
        
        for index, article in enumerate(articles, 1):
            article_id = article['id']
            url = article['src_url']
            
            print(f"\n[{index}/{len(articles)}] æ­£åœ¨å¤„ç†: {url}")
            
            try:
                # æŠ“å–æ–‡ç« 
                article_info = self.fetch_article(url)
                
                # ä¿å­˜åˆ°æ•°æ®åº“
                if self.save_article_to_db(article_id, url, article_info):
                    success_count += 1
                else:
                    failed_count += 1
                
                # éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«åçˆ¬è™«æ‹¦æˆª
                if index < len(articles):
                    delay = random.uniform(2, 5)
                    print(f"â³ ç­‰å¾… {delay:.1f} ç§’åç»§ç»­...")
                    time.sleep(delay)
                    
            except KeyboardInterrupt:
                print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
                break
            except Exception as e:
                print(f"âœ— æŠ“å–å¤±è´¥: {str(e)}")
                failed_count += 1
                # å¤±è´¥åä¹Ÿå»¶è¿Ÿä¸€ä¸‹
                time.sleep(2)
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        print(f"\n{'='*70}")
        print(f"æŠ“å–å®Œæˆ!")
        print(f"æˆåŠŸ: {success_count} ç¯‡")
        print(f"å¤±è´¥: {failed_count} ç¯‡")
        print(f"æ€»è®¡: {len(articles)} ç¯‡")
        print(f"{'='*70}\n")


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("=" * 70)
        print("çŸ¥ä¹æ–‡ç« æŠ“å–å·¥å…· - å‡çº§ç‰ˆï¼ˆæ”¯æŒæ•°æ®åº“ï¼‰")
        print("=" * 70)
        print("\nä½¿ç”¨æ–¹æ³•:")
        print("  # å•ç¯‡æŠ“å–æ¨¡å¼")
        print("  python3 zhihu_crawler.py <URL> [è¾“å‡ºæ–‡ä»¶] [--cookie COOKIE]")
        print("\n  # æ‰¹é‡æŠ“å–æ¨¡å¼ï¼ˆä»æ•°æ®åº“ï¼‰")
        print("  python3 zhihu_crawler.py --batch [æ•°é‡] [--cookie COOKIE]")
        print("\næç¤º:")
        print("  - å¯å°† Cookie ä¿å­˜åˆ° cookie.txt æ–‡ä»¶ï¼Œç¨‹åºä¼šè‡ªåŠ¨è¯»å–")
        print("  - æˆ–ä½¿ç”¨ --cookie å‚æ•°æŒ‡å®š Cookie")
        print("\nç¤ºä¾‹:")
        print("  # åŸºç¡€æŠ“å–")
        print("  python3 zhihu_crawler.py https://zhuanlan.zhihu.com/p/123456")
        print("\n  # æŒ‡å®šè¾“å‡ºæ–‡ä»¶")
        print("  python3 zhihu_crawler.py https://zhuanlan.zhihu.com/p/123456 output.html")
        print("\n  # ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šCookie")
        print("  python3 zhihu_crawler.py https://zhuanlan.zhihu.com/p/123456 --cookie 'xxx'")
        print("\n  # æ‰¹é‡æŠ“å–10ç¯‡æ–‡ç« åˆ°æ•°æ®åº“")
        print("  python3 zhihu_crawler.py --batch 10")
        print("\nè·å–Cookieçš„æ–¹æ³•:")
        print("  1. åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€çŸ¥ä¹å¹¶ç™»å½•")
        print("  2. æŒ‰F12æ‰“å¼€å¼€å‘è€…å·¥å…·ï¼Œåˆ‡æ¢åˆ°Networkæ ‡ç­¾")
        print("  3. åˆ·æ–°é¡µé¢ï¼Œæ‰¾åˆ°æ–‡ç« è¯·æ±‚ï¼Œå¤åˆ¶Cookieå€¼")
        print("  4. ä¿å­˜åˆ° cookie.txt æˆ–ä½¿ç”¨ --cookie å‚æ•°")
        print("=" * 70)
        sys.exit(1)
    
    # è§£æå‚æ•°
    url = None
    output_file = None
    cookie = None
    batch_mode = False
    batch_limit = 10
    
    i = 1
    while i < len(sys.argv):
        arg = sys.argv[i]
        
        if arg == '--batch':
            batch_mode = True
            if i + 1 < len(sys.argv) and not sys.argv[i + 1].startswith('--'):
                try:
                    batch_limit = int(sys.argv[i + 1])
                    i += 2
                    continue
                except ValueError:
                    pass
            i += 1
        elif arg == '--cookie':
            if i + 1 < len(sys.argv):
                cookie = sys.argv[i + 1]
                i += 2
            else:
                print("âœ— é”™è¯¯: --cookie éœ€è¦æä¾›Cookieå€¼")
                sys.exit(1)
        elif arg.startswith('--'):
            print(f"âœ— é”™è¯¯: æœªçŸ¥é€‰é¡¹ {arg}")
            sys.exit(1)
        else:
            if not url:
                url = arg
            elif not output_file:
                output_file = arg
            i += 1
    
    # å¦‚æœæ²¡æœ‰æä¾› Cookieï¼Œå°è¯•ä» cookie.txt è¯»å–
    if not cookie:
        cookie_file = 'cookie.txt'
        if os.path.exists(cookie_file):
            try:
                with open(cookie_file, 'r', encoding='utf-8') as f:
                    cookie_content = f.read().strip()
                    if cookie_content:
                        cookie = cookie_content
                        print(f"âœ“ å·²ä» {cookie_file} è¯»å– Cookie")
                    else:
                        print(f"âš ï¸  {cookie_file} ä¸ºç©ºï¼Œè¯·æ·»åŠ æœ‰æ•ˆçš„ Cookie")
            except Exception as e:
                print(f"âš ï¸  è¯»å– {cookie_file} å¤±è´¥: {str(e)}")
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ° {cookie_file}ï¼Œè¯·æä¾› --cookie å‚æ•°æˆ–åˆ›å»º cookie.txt æ–‡ä»¶")
    
    # æ‰¹é‡æ¨¡å¼
    if batch_mode:
        db_config = {
            'host': '172.105.225.120',
            'user': 'root',
            'password': 'lnmp.org#25295',
            'database': 'wordpress',
            'port': 3306
        }
        
        crawler = ZhihuArticleCrawler(cookie=cookie, db_config=db_config)
        
        try:
            crawler.batch_crawl_and_save(batch_limit)
            print("\nâœ… æ‰¹é‡æŠ“å–å®Œæˆï¼")
        except KeyboardInterrupt:
            print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
            sys.exit(130)
        except Exception as e:
            print(f"\nâœ— é”™è¯¯: {str(e)}")
            sys.exit(1)
    
    # å•ç¯‡æ¨¡å¼
    else:
        if not url:
            print("âœ— é”™è¯¯: è¯·æä¾›URL")
            sys.exit(1)
        
        crawler = ZhihuArticleCrawler(cookie=cookie)
        
        try:
            crawler.save_to_html(url, output_file)
            print("\nâœ… æŠ“å–æˆåŠŸï¼")
        except KeyboardInterrupt:
            print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
            sys.exit(130)
        except Exception as e:
            print(f"\nâœ— é”™è¯¯: {str(e)}")
            if '403' in str(e) or 'Cookie' in str(e):
                print("\nğŸ’¡ æç¤º: éœ€è¦æä¾›æœ‰æ•ˆçš„Cookie")
                print("   è¯·åœ¨ cookie.txt ä¸­æ·»åŠ  Cookie æˆ–ä½¿ç”¨ --cookie å‚æ•°")
            sys.exit(1)


if __name__ == '__main__':
    main()
