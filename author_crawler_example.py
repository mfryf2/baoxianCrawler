#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çŸ¥ä¹ä½œè€…æ–‡ç« åˆ—è¡¨æŠ“å– - ä½¿ç”¨ç¤ºä¾‹
"""

import os
from zhihu_author_crawler import ZhihuAuthorCrawler

def example_basic_usage():
    """åŸºç¡€ä½¿ç”¨ç¤ºä¾‹"""
    print("=" * 80)
    print("ç¤ºä¾‹1: åŸºç¡€ä½¿ç”¨ - è·å–ä½œè€…æ‰€æœ‰æ–‡ç« ")
    print("=" * 80)
    
    # ä½œè€…URL - å¥¶çˆ¸ä¿é™©
    author_url = "https://www.zhihu.com/org/nai-ba-bao-25/posts"
    
    # è¯»å–Cookieï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    cookie = None
    if os.path.exists('cookie.txt'):
        with open('cookie.txt', 'r', encoding='utf-8') as f:
            cookie = f.read().strip()
        print("âœ“ å·²åŠ è½½Cookie")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = ZhihuAuthorCrawler(cookie=cookie)
    
    try:
        # è·å–æ‰€æœ‰æ–‡ç« ï¼ˆè¿™é‡Œé™åˆ¶ä¸º50ç¯‡ä½œä¸ºç¤ºä¾‹ï¼‰
        print(f"æ­£åœ¨è·å–ä½œè€…æ–‡ç« : {author_url}")
        data = crawler.fetch_author_articles(
            author_url,
            max_articles=50,  # é™åˆ¶50ç¯‡ï¼Œå®é™…ä½¿ç”¨æ—¶å¯ä»¥å»æ‰è¿™ä¸ªå‚æ•°è·å–æ‰€æœ‰æ–‡ç« 
            delay=1.0  # è¯·æ±‚é—´éš”1ç§’
        )
        
        if data:
            print(f"\nâœ… è·å–æˆåŠŸï¼")
            print(f"ä½œè€…: {data['author']['name']}")
            print(f"æ–‡ç« æ•°é‡: {data['total_fetched']}")
            
            # ä¿å­˜ä¸ºå¤šç§æ ¼å¼
            crawler.save_to_json(data)
            crawler.save_to_txt(data)
            crawler.save_to_csv(data)
            
            return True
        else:
            print("âŒ è·å–å¤±è´¥")
            return False
            
    except Exception as e:
        print(f"âŒ é”™è¯¯: {str(e)}")
        return False


def example_limited_articles():
    """é™é‡è·å–ç¤ºä¾‹"""
    print("\n" + "=" * 80)
    print("ç¤ºä¾‹2: é™é‡è·å– - åªè·å–æœ€æ–°çš„20ç¯‡æ–‡ç« ")
    print("=" * 80)
    
    author_url = "https://www.zhihu.com/org/nai-ba-bao-25/posts"
    
    cookie = None
    if os.path.exists('cookie.txt'):
        with open('cookie.txt', 'r', encoding='utf-8') as f:
            cookie = f.read().strip()
    
    crawler = ZhihuAuthorCrawler(cookie=cookie)
    
    try:
        # åªè·å–æœ€æ–°çš„20ç¯‡æ–‡ç« 
        data = crawler.fetch_author_articles(
            author_url,
            max_articles=20,
            delay=0.5  # å‡å°‘å»¶è¿Ÿ
        )
        
        if data:
            print(f"\nâœ… è·å–æˆåŠŸï¼è·å–äº† {data['total_fetched']} ç¯‡æ–‡ç« ")
            
            # æ˜¾ç¤ºæ–‡ç« åˆ—è¡¨
            print("\næœ€æ–°æ–‡ç« åˆ—è¡¨:")
            for i, article in enumerate(data['articles'][:10], 1):  # æ˜¾ç¤ºå‰10ç¯‡
                print(f"{i:2d}. {article['title']}")
                print(f"     {article['url']}")
                if article['created_time']:
                    print(f"     å‘å¸ƒæ—¶é—´: {article['created_time']}")
                if article['voteup_count'] > 0:
                    print(f"     ç‚¹èµæ•°: {article['voteup_count']}")
                print()
            
            # åªä¿å­˜ä¸ºJSONæ ¼å¼
            json_file = crawler.save_to_json(data, "latest_20_articles.json")
            
            return True
        else:
            return False
            
    except Exception as e:
        print(f"âŒ é”™è¯¯: {str(e)}")
        return False


def example_custom_output():
    """è‡ªå®šä¹‰è¾“å‡ºç¤ºä¾‹"""
    print("\n" + "=" * 80)
    print("ç¤ºä¾‹3: è‡ªå®šä¹‰è¾“å‡º - æŒ‡å®šæ–‡ä»¶åå’Œæ ¼å¼")
    print("=" * 80)
    
    author_url = "https://www.zhihu.com/org/nai-ba-bao-25/posts"
    
    cookie = None
    if os.path.exists('cookie.txt'):
        with open('cookie.txt', 'r', encoding='utf-8') as f:
            cookie = f.read().strip()
    
    crawler = ZhihuAuthorCrawler(cookie=cookie)
    
    try:
        # è·å–æ–‡ç« 
        data = crawler.fetch_author_articles(
            author_url,
            max_articles=10,  # å°‘é‡æ–‡ç« ç”¨äºæ¼”ç¤º
            delay=0.5
        )
        
        if data:
            print(f"\nâœ… è·å–æˆåŠŸï¼")
            
            # è‡ªå®šä¹‰æ–‡ä»¶åä¿å­˜
            crawler.save_to_json(data, "naibaobao_articles.json")
            crawler.save_to_txt(data, "naibaobao_articles.txt")
            crawler.save_to_csv(data, "naibaobao_articles.csv")
            
            print("\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
            print(f"ä½œè€…: {data['author']['name']}")
            print(f"æ–‡ç« æ€»æ•°: {data['total_fetched']}")
            
            # ç»Ÿè®¡ç‚¹èµæ•°
            total_likes = sum(article['voteup_count'] for article in data['articles'])
            print(f"æ€»ç‚¹èµæ•°: {total_likes}")
            
            # æ‰¾å‡ºæœ€å—æ¬¢è¿çš„æ–‡ç« 
            if data['articles']:
                most_liked = max(data['articles'], key=lambda x: x['voteup_count'])
                print(f"æœ€å—æ¬¢è¿æ–‡ç« : {most_liked['title']} ({most_liked['voteup_count']} èµ)")
            
            return True
        else:
            return False
            
    except Exception as e:
        print(f"âŒ é”™è¯¯: {str(e)}")
        return False


def example_multiple_authors():
    """å¤šä¸ªä½œè€…ç¤ºä¾‹"""
    print("\n" + "=" * 80)
    print("ç¤ºä¾‹4: æ‰¹é‡å¤„ç† - è·å–å¤šä¸ªä½œè€…çš„æ–‡ç« ")
    print("=" * 80)
    
    # å¤šä¸ªä½œè€…URLï¼ˆè¿™é‡Œåªç”¨ä¸€ä¸ªä½œä¸ºç¤ºä¾‹ï¼‰
    authors = [
        {
            'name': 'å¥¶çˆ¸ä¿é™©',
            'url': 'https://www.zhihu.com/org/nai-ba-bao-25/posts'
        }
        # å¯ä»¥æ·»åŠ æ›´å¤šä½œè€…
        # {
        #     'name': 'å…¶ä»–ä½œè€…',
        #     'url': 'https://www.zhihu.com/people/other-author/posts'
        # }
    ]
    
    cookie = None
    if os.path.exists('cookie.txt'):
        with open('cookie.txt', 'r', encoding='utf-8') as f:
            cookie = f.read().strip()
    
    crawler = ZhihuAuthorCrawler(cookie=cookie)
    
    results = []
    
    for author_info in authors:
        try:
            print(f"\næ­£åœ¨å¤„ç†: {author_info['name']}")
            
            data = crawler.fetch_author_articles(
                author_info['url'],
                max_articles=5,  # æ¯ä¸ªä½œè€…åªè·å–5ç¯‡ä½œä¸ºç¤ºä¾‹
                delay=1.0
            )
            
            if data:
                # ä½¿ç”¨ä½œè€…åç§°ä½œä¸ºæ–‡ä»¶åå‰ç¼€
                safe_name = "".join(c for c in author_info['name'] if c.isalnum() or c in (' ', '-', '_')).strip()
                crawler.save_to_json(data, f"{safe_name}_articles.json")
                
                results.append({
                    'author': author_info['name'],
                    'articles_count': data['total_fetched'],
                    'success': True
                })
                
                print(f"âœ… {author_info['name']}: è·å–äº† {data['total_fetched']} ç¯‡æ–‡ç« ")
            else:
                results.append({
                    'author': author_info['name'],
                    'articles_count': 0,
                    'success': False
                })
                print(f"âŒ {author_info['name']}: è·å–å¤±è´¥")
                
        except Exception as e:
            print(f"âŒ {author_info['name']}: {str(e)}")
            results.append({
                'author': author_info['name'],
                'articles_count': 0,
                'success': False
            })
    
    # æ€»ç»“
    print(f"\nğŸ“Š æ‰¹é‡å¤„ç†ç»“æœ:")
    total_articles = 0
    success_count = 0
    
    for result in results:
        status = "âœ…" if result['success'] else "âŒ"
        print(f"{status} {result['author']}: {result['articles_count']} ç¯‡æ–‡ç« ")
        if result['success']:
            total_articles += result['articles_count']
            success_count += 1
    
    print(f"\næ€»è®¡: {success_count}/{len(authors)} ä¸ªä½œè€…æˆåŠŸï¼Œå…±è·å– {total_articles} ç¯‡æ–‡ç« ")
    
    return success_count > 0


def main():
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("çŸ¥ä¹ä½œè€…æ–‡ç« åˆ—è¡¨æŠ“å–å·¥å…· - ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 80)
    
    # æ£€æŸ¥Cookie
    if not os.path.exists('cookie.txt'):
        print("\nâš ï¸  è­¦å‘Š: æœªæ‰¾åˆ° cookie.txt æ–‡ä»¶")
        print("å»ºè®®å…ˆè¿è¡Œ: python3 get_cookie_helper.py")
        print("\næ²¡æœ‰Cookieå¯èƒ½ä¼šé‡åˆ°è®¿é—®é™åˆ¶...")
        
        response = input("\næ˜¯å¦ç»§ç»­è¿è¡Œç¤ºä¾‹ï¼Ÿ(y/n): ").strip().lower()
        if response != 'y':
            print("å·²å–æ¶ˆ")
            return
    
    # è¿è¡Œç¤ºä¾‹
    examples = [
        ("åŸºç¡€ä½¿ç”¨", example_basic_usage),
        ("é™é‡è·å–", example_limited_articles),
        ("è‡ªå®šä¹‰è¾“å‡º", example_custom_output),
        ("æ‰¹é‡å¤„ç†", example_multiple_authors),
    ]
    
    for name, func in examples:
        try:
            print(f"\n{'='*20} è¿è¡Œ {name} {'='*20}")
            success = func()
            if success:
                print(f"âœ… {name} ç¤ºä¾‹è¿è¡ŒæˆåŠŸ")
            else:
                print(f"âŒ {name} ç¤ºä¾‹è¿è¡Œå¤±è´¥")
        except KeyboardInterrupt:
            print(f"\nâš ï¸  ç”¨æˆ·ä¸­æ–­äº† {name} ç¤ºä¾‹")
            break
        except Exception as e:
            print(f"âŒ {name} ç¤ºä¾‹å‡ºé”™: {str(e)}")
    
    print("\n" + "=" * 80)
    print("æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
    print("\nğŸ’¡ æç¤º:")
    print("1. å®é™…ä½¿ç”¨æ—¶ï¼Œå¯ä»¥å»æ‰ max_articles å‚æ•°è·å–æ‰€æœ‰æ–‡ç« ")
    print("2. å¯ä»¥è°ƒæ•´ delay å‚æ•°æ§åˆ¶è¯·æ±‚é¢‘ç‡")
    print("3. å»ºè®®ä½¿ç”¨æœ‰æ•ˆçš„Cookieä»¥é¿å…è®¿é—®é™åˆ¶")
    print("4. å¤§é‡æ•°æ®æŠ“å–æ—¶è¯·æ³¨æ„éµå®ˆç½‘ç«™çš„ä½¿ç”¨æ¡æ¬¾")
    print("=" * 80)


if __name__ == '__main__':
    main()