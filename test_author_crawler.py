#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•çŸ¥ä¹ä½œè€…æ–‡ç« åˆ—è¡¨æŠ“å–åŠŸèƒ½
"""

import sys
import os
from datetime import datetime

# æµ‹è¯•ä½œè€…URL
TEST_AUTHOR_URL = "https://www.zhihu.com/org/nai-ba-bao-25/posts"

def test_author_info():
    """æµ‹è¯•è·å–ä½œè€…ä¿¡æ¯"""
    print("=" * 70)
    print("æµ‹è¯•1: è·å–ä½œè€…ä¿¡æ¯")
    print("=" * 70)
    
    from zhihu_author_crawler import ZhihuAuthorCrawler
    
    # è¯»å–Cookieï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    cookie = None
    if os.path.exists('cookie.txt'):
        with open('cookie.txt', 'r', encoding='utf-8') as f:
            cookie = f.read().strip()
        print("âœ“ å·²åŠ è½½Cookie")
    else:
        print("âš ï¸  æœªæ‰¾åˆ°cookie.txtï¼Œå°†ä¸ä½¿ç”¨Cookie")
    
    crawler = ZhihuAuthorCrawler(cookie=cookie)
    
    try:
        print(f"\næ­£åœ¨æµ‹è¯•ä½œè€…URL: {TEST_AUTHOR_URL}")
        author_info = crawler._extract_author_info(TEST_AUTHOR_URL)
        
        print(f"\nâœ… è·å–ä½œè€…ä¿¡æ¯æˆåŠŸï¼")
        print(f"ä½œè€…åç§°: {author_info['name']}")
        print(f"ä½œè€…ID: {author_info['id']}")
        print(f"ä½œè€…é¡µé¢: {author_info['url']}")
        print(f"é¢„è®¡æ–‡ç« æ•°: {author_info['total_posts']}")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        return False


def test_fetch_limited_articles():
    """æµ‹è¯•è·å–é™é‡æ–‡ç« """
    print("\n" + "=" * 70)
    print("æµ‹è¯•2: è·å–å‰10ç¯‡æ–‡ç« ")
    print("=" * 70)
    
    from zhihu_author_crawler import ZhihuAuthorCrawler
    
    # è¯»å–Cookie
    cookie = None
    if os.path.exists('cookie.txt'):
        with open('cookie.txt', 'r', encoding='utf-8') as f:
            cookie = f.read().strip()
    
    crawler = ZhihuAuthorCrawler(cookie=cookie)
    
    try:
        data = crawler.fetch_author_articles(
            TEST_AUTHOR_URL,
            max_articles=10,  # åªè·å–å‰10ç¯‡
            delay=0.5  # å‡å°‘å»¶è¿ŸåŠ å¿«æµ‹è¯•
        )
        
        if data:
            print(f"\nâœ… è·å–æ–‡ç« æˆåŠŸï¼")
            print(f"ä½œè€…: {data['author']['name']}")
            print(f"è·å–æ–‡ç« æ•°: {data['total_fetched']}")
            
            # æ˜¾ç¤ºå‰3ç¯‡æ–‡ç« 
            print(f"\nå‰3ç¯‡æ–‡ç« :")
            for i, article in enumerate(data['articles'][:3], 1):
                print(f"{i}. {article['title']}")
                print(f"   URL: {article['url']}")
                if article['created_time']:
                    print(f"   å‘å¸ƒæ—¶é—´: {article['created_time']}")
                print()
            
            # ä¿å­˜æµ‹è¯•æ•°æ®
            output_file = f"test_author_articles_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            crawler.save_to_json(data, output_file)
            
            return True
        else:
            print(f"\nâŒ è·å–å¤±è´¥")
            return False
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        return False


def test_save_formats():
    """æµ‹è¯•ä¸åŒä¿å­˜æ ¼å¼"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•3: æµ‹è¯•ä¿å­˜æ ¼å¼")
    print("=" * 70)
    
    from zhihu_author_crawler import ZhihuAuthorCrawler
    
    # è¯»å–Cookie
    cookie = None
    if os.path.exists('cookie.txt'):
        with open('cookie.txt', 'r', encoding='utf-8') as f:
            cookie = f.read().strip()
    
    crawler = ZhihuAuthorCrawler(cookie=cookie)
    
    try:
        # è·å–å°‘é‡æ–‡ç« ç”¨äºæµ‹è¯•
        data = crawler.fetch_author_articles(
            TEST_AUTHOR_URL,
            max_articles=5,
            delay=0.5
        )
        
        if not data:
            print("âŒ è·å–æ–‡ç« å¤±è´¥")
            return False
        
        print(f"\nâœ… è·å–åˆ° {data['total_fetched']} ç¯‡æ–‡ç« ï¼Œæµ‹è¯•ä¿å­˜æ ¼å¼...")
        
        # æµ‹è¯•ä¿å­˜ä¸ºä¸åŒæ ¼å¼
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        json_file = crawler.save_to_json(data, f"test_format_{timestamp}.json")
        txt_file = crawler.save_to_txt(data, f"test_format_{timestamp}.txt")
        csv_file = crawler.save_to_csv(data, f"test_format_{timestamp}.csv")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åˆ›å»ºæˆåŠŸ
        files_created = []
        for file_path in [json_file, txt_file, csv_file]:
            if os.path.exists(file_path):
                size = os.path.getsize(file_path)
                files_created.append(f"{file_path} ({size} bytes)")
        
        print(f"\nâœ… æˆåŠŸåˆ›å»º {len(files_created)} ä¸ªæ–‡ä»¶:")
        for file_info in files_created:
            print(f"  - {file_info}")
        
        return len(files_created) == 3
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        return False


def test_api_url_generation():
    """æµ‹è¯•API URLç”Ÿæˆ"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•4: API URLç”Ÿæˆ")
    print("=" * 70)
    
    from zhihu_author_crawler import ZhihuAuthorCrawler
    
    crawler = ZhihuAuthorCrawler()
    
    test_cases = [
        ("https://www.zhihu.com/org/nai-ba-bao-25/posts", "æœºæ„è´¦å·"),
        ("https://www.zhihu.com/people/test-user/posts", "ä¸ªäººè´¦å·"),
    ]
    
    success_count = 0
    
    for url, desc in test_cases:
        try:
            api_url = crawler._get_api_url(url)
            print(f"âœ… {desc}: {url}")
            print(f"   API URL: {api_url}")
            success_count += 1
        except Exception as e:
            print(f"âŒ {desc}: {url}")
            print(f"   é”™è¯¯: {str(e)}")
    
    print(f"\nâœ… API URLç”Ÿæˆæµ‹è¯•: {success_count}/{len(test_cases)} é€šè¿‡")
    return success_count == len(test_cases)


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "=" * 70)
    print("çŸ¥ä¹ä½œè€…æ–‡ç« åˆ—è¡¨æŠ“å–å·¥å…· - åŠŸèƒ½æµ‹è¯•")
    print("=" * 70)
    
    # æ£€æŸ¥Cookie
    if not os.path.exists('cookie.txt'):
        print("\nâš ï¸  è­¦å‘Š: æœªæ‰¾åˆ° cookie.txt æ–‡ä»¶")
        print("å»ºè®®å…ˆè¿è¡Œ: python3 get_cookie_helper.py -v")
        print("\nç»§ç»­æµ‹è¯•å¯èƒ½ä¼šé‡åˆ°403é”™è¯¯...")
        
        response = input("\næ˜¯å¦ç»§ç»­æµ‹è¯•ï¼Ÿ(y/n): ").strip().lower()
        if response != 'y':
            print("å·²å–æ¶ˆ")
            return
    
    results = []
    
    # è¿è¡Œæµ‹è¯•
    results.append(("API URLç”Ÿæˆ", test_api_url_generation()))
    results.append(("è·å–ä½œè€…ä¿¡æ¯", test_author_info()))
    results.append(("è·å–é™é‡æ–‡ç« ", test_fetch_limited_articles()))
    results.append(("ä¿å­˜æ ¼å¼æµ‹è¯•", test_save_formats()))
    
    # æ€»ç»“
    print("\n" + "=" * 70)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 70)
    
    for name, passed in results:
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"{name}: {status}")
    
    total = len(results)
    passed = sum(1 for _, p in results if p)
    
    print(f"\næ€»è®¡: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("\nä½¿ç”¨ç¤ºä¾‹:")
        print("python3 zhihu_author_crawler.py https://www.zhihu.com/org/nai-ba-bao-25/posts")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥Cookieæˆ–ç½‘ç»œè¿æ¥")
    
    print("=" * 70)


if __name__ == '__main__':
    main()