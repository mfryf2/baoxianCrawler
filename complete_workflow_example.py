#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´å·¥ä½œæµç¨‹æ¼”ç¤ºï¼šä»è·å–ä½œè€…æ–‡ç« åˆ—è¡¨åˆ°æ‰¹é‡æŠ“å–æ–‡ç« å†…å®¹
"""

import os
import sys
import time
import subprocess
from datetime import datetime


def run_command(command, description=""):
    """è¿è¡Œå‘½ä»¤å¹¶æ˜¾ç¤ºç»“æœ"""
    if description:
        print(f"\n{'='*60}")
        print(f"æ­¥éª¤: {description}")
        print(f"{'='*60}")
        print(f"æ‰§è¡Œå‘½ä»¤: {command}")
    
    try:
        result = subprocess.run(command, shell=True, capture_output=True, text=True, encoding='utf-8')
        
        if result.stdout:
            print("è¾“å‡º:")
            print(result.stdout)
        
        if result.stderr:
            print("é”™è¯¯ä¿¡æ¯:")
            print(result.stderr)
        
        return result.returncode == 0
        
    except Exception as e:
        print(f"âŒ å‘½ä»¤æ‰§è¡Œå¤±è´¥: {str(e)}")
        return False


def complete_workflow_demo():
    """å®Œæ•´å·¥ä½œæµç¨‹æ¼”ç¤º"""
    print("ğŸ¯ çŸ¥ä¹ä½œè€…æ–‡ç« æŠ“å–å®Œæ•´å·¥ä½œæµç¨‹æ¼”ç¤º")
    print("=" * 80)
    
    # é…ç½®å‚æ•°
    author_url = "https://www.zhihu.com/org/nai-ba-bao-25/posts"
    max_articles = 5  # æ¼”ç¤ºç”¨ï¼Œå®é™…å¯ä»¥è®¾ç½®æ›´å¤§
    max_crawl = 3     # æ¼”ç¤ºæŠ“å–å‰3ç¯‡æ–‡ç« 
    
    print(f"ç›®æ ‡ä½œè€…: {author_url}")
    print(f"è·å–æ–‡ç« æ•°: {max_articles}")
    print(f"æŠ“å–æ–‡ç« æ•°: {max_crawl}")
    
    # æ£€æŸ¥Cookie
    if not os.path.exists('cookie.txt'):
        print("\nâš ï¸  è­¦å‘Š: æœªæ‰¾åˆ° cookie.txt æ–‡ä»¶")
        print("å»ºè®®å…ˆè¿è¡Œ: python3 get_cookie_helper.py")
        
        response = input("\næ˜¯å¦ç»§ç»­æ¼”ç¤ºï¼Ÿ(y/n): ").strip().lower()
        if response != 'y':
            print("å·²å–æ¶ˆ")
            return False
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # æ­¥éª¤1: è·å–ä½œè€…æ–‡ç« åˆ—è¡¨
    step1_success = run_command(
        f'python3 zhihu_author_crawler.py "{author_url}" --max-articles {max_articles} --format json --output demo_{timestamp}',
        "1. è·å–ä½œè€…æ–‡ç« åˆ—è¡¨"
    )
    
    if not step1_success:
        print("âŒ æ­¥éª¤1å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
        return False
    
    # æŸ¥æ‰¾ç”Ÿæˆçš„JSONæ–‡ä»¶
    json_file = f"demo_{timestamp}.json"
    if not os.path.exists(json_file):
        print(f"âŒ æœªæ‰¾åˆ°ç”Ÿæˆçš„JSONæ–‡ä»¶: {json_file}")
        return False
    
    time.sleep(1)
    
    # æ­¥éª¤2: ä»JSONæå–URLåˆ—è¡¨
    step2_success = run_command(
        f'python3 extract_urls_from_json.py {json_file} --output demo_urls_{timestamp}.txt --max-urls {max_crawl}',
        "2. æå–æ–‡ç« URLåˆ—è¡¨"
    )
    
    if not step2_success:
        print("âŒ æ­¥éª¤2å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
        return False
    
    urls_file = f"demo_urls_{timestamp}.txt"
    if not os.path.exists(urls_file):
        print(f"âŒ æœªæ‰¾åˆ°ç”Ÿæˆçš„URLæ–‡ä»¶: {urls_file}")
        return False
    
    time.sleep(1)
    
    # æ­¥éª¤3: æ‰¹é‡æŠ“å–æ–‡ç« å†…å®¹
    cookie_param = ""
    if os.path.exists('cookie.txt'):
        cookie_param = "--cookie $(cat cookie.txt)"
    
    step3_success = run_command(
        f'python3 zhihu_crawler.py --batch {urls_file} --output-dir demo_articles_{timestamp} --workers 2 --delay 1.0 {cookie_param}',
        "3. æ‰¹é‡æŠ“å–æ–‡ç« å†…å®¹"
    )
    
    # æ­¥éª¤4: æ˜¾ç¤ºç»“æœæ€»ç»“
    print(f"\n{'='*60}")
    print("4. å·¥ä½œæµç¨‹æ€»ç»“")
    print(f"{'='*60}")
    
    # ç»Ÿè®¡ç”Ÿæˆçš„æ–‡ä»¶
    generated_files = []
    
    # JSONæ–‡ä»¶
    if os.path.exists(json_file):
        size = os.path.getsize(json_file)
        generated_files.append(f"ğŸ“„ {json_file} ({size} bytes) - æ–‡ç« åˆ—è¡¨æ•°æ®")
    
    # URLæ–‡ä»¶
    if os.path.exists(urls_file):
        with open(urls_file, 'r', encoding='utf-8') as f:
            url_count = len(f.readlines())
        generated_files.append(f"ğŸ”— {urls_file} ({url_count} URLs) - æ–‡ç« é“¾æ¥åˆ—è¡¨")
    
    # æ–‡ç« ç›®å½•
    articles_dir = f"demo_articles_{timestamp}"
    if os.path.exists(articles_dir):
        article_files = [f for f in os.listdir(articles_dir) if f.endswith('.html')]
        generated_files.append(f"ğŸ“ {articles_dir}/ ({len(article_files)} ç¯‡æ–‡ç« ) - æŠ“å–çš„æ–‡ç« å†…å®¹")
    
    if generated_files:
        print("âœ… ç”Ÿæˆçš„æ–‡ä»¶:")
        for file_info in generated_files:
            print(f"  {file_info}")
    else:
        print("âŒ æœªç”Ÿæˆä»»ä½•æ–‡ä»¶")
    
    # æ˜¾ç¤ºä½¿ç”¨å»ºè®®
    print(f"\nğŸ’¡ åç»­æ“ä½œå»ºè®®:")
    print(f"1. æŸ¥çœ‹æ–‡ç« åˆ—è¡¨: cat {json_file}")
    print(f"2. æŸ¥çœ‹URLåˆ—è¡¨: cat {urls_file}")
    if os.path.exists(articles_dir):
        print(f"3. æŸ¥çœ‹æŠ“å–çš„æ–‡ç« : ls {articles_dir}/")
        print(f"4. åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æ–‡ç« : open {articles_dir}/*.html")
    
    # æ¸…ç†é€‰é¡¹
    print(f"\nğŸ§¹ æ¸…ç†æ¼”ç¤ºæ–‡ä»¶:")
    print(f"rm -f demo_{timestamp}.* && rm -rf demo_articles_{timestamp}/")
    
    return True


def main():
    """ä¸»å‡½æ•°"""
    print("çŸ¥ä¹ä½œè€…æ–‡ç« æŠ“å– - å®Œæ•´å·¥ä½œæµç¨‹æ¼”ç¤º")
    print("=" * 80)
    print("\nè¿™ä¸ªæ¼”ç¤ºå°†å±•ç¤ºå®Œæ•´çš„å·¥ä½œæµç¨‹:")
    print("1. è·å–ä½œè€…æ–‡ç« åˆ—è¡¨")
    print("2. æå–æ–‡ç« URL")
    print("3. æ‰¹é‡æŠ“å–æ–‡ç« å†…å®¹")
    print("4. æŸ¥çœ‹ç»“æœ")
    
    response = input("\næ˜¯å¦å¼€å§‹æ¼”ç¤ºï¼Ÿ(y/n): ").strip().lower()
    if response != 'y':
        print("å·²å–æ¶ˆ")
        return
    
    try:
        success = complete_workflow_demo()
        
        if success:
            print(f"\nğŸ‰ å·¥ä½œæµç¨‹æ¼”ç¤ºå®Œæˆï¼")
            print(f"\nğŸ“š æ›´å¤šä½¿ç”¨æ–¹æ³•:")
            print(f"- æŸ¥çœ‹è¯¦ç»†æŒ‡å—: cat AUTHOR_CRAWLER_GUIDE.md")
            print(f"- è¿è¡Œæµ‹è¯•: python3 test_author_crawler.py")
            print(f"- æŸ¥çœ‹ç¤ºä¾‹: python3 author_crawler_example.py")
        else:
            print(f"\nâš ï¸  æ¼”ç¤ºè¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        
    except KeyboardInterrupt:
        print(f"\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ¼”ç¤º")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")


if __name__ == '__main__':
    main()