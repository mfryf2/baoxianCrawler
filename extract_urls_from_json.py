#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä»ä½œè€…æ–‡ç« JSONæ–‡ä»¶ä¸­æå–URLåˆ—è¡¨
ç”¨äºåç»­æ‰¹é‡æŠ“å–æ–‡ç« å†…å®¹
"""

import json
import sys
import os
from datetime import datetime


def extract_urls_from_json(json_file, output_file=None, max_urls=None):
    """
    ä»JSONæ–‡ä»¶ä¸­æå–æ–‡ç« URL
    
    Args:
        json_file: JSONæ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        max_urls: æœ€å¤§URLæ•°é‡é™åˆ¶
        
    Returns:
        list: URLåˆ—è¡¨
    """
    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if 'articles' not in data:
            print(f"âŒ JSONæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œæœªæ‰¾åˆ°articleså­—æ®µ")
            return []
        
        articles = data['articles']
        urls = []
        
        for article in articles:
            if 'url' in article and article['url']:
                url = article['url']
                # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
                if url.startswith('http://'):
                    url = url.replace('http://', 'https://')
                elif not url.startswith('https://'):
                    url = 'https://www.zhihu.com' + url
                
                urls.append(url)
                
                if max_urls and len(urls) >= max_urls:
                    break
        
        # ä¿å­˜URLåˆ—è¡¨
        if not output_file:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            author_name = data.get('author', {}).get('name', 'unknown')
            safe_name = "".join(c for c in author_name if c.isalnum() or c in (' ', '-', '_')).strip()
            safe_name = safe_name[:20]  # é™åˆ¶æ–‡ä»¶åé•¿åº¦
            output_file = f"{safe_name}_urls_{timestamp}.txt"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            for url in urls:
                f.write(url + '\n')
        
        print(f"âœ… æˆåŠŸæå– {len(urls)} ä¸ªURL")
        print(f"âœ… å·²ä¿å­˜åˆ°: {output_file}")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        if 'author' in data:
            author_info = data['author']
            print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"ä½œè€…: {author_info.get('name', 'æœªçŸ¥')}")
            print(f"æ€»æ–‡ç« æ•°: {data.get('total_fetched', 0)}")
            print(f"æå–URLæ•°: {len(urls)}")
        
        return urls
        
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {json_file}")
        return []
    except json.JSONDecodeError:
        print(f"âŒ JSONæ–‡ä»¶æ ¼å¼é”™è¯¯: {json_file}")
        return []
    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return []


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("=" * 70)
        print("ä»ä½œè€…æ–‡ç« JSONæ–‡ä»¶ä¸­æå–URLåˆ—è¡¨")
        print("=" * 70)
        print("\nä½¿ç”¨æ–¹æ³•:")
        print("  python3 extract_urls_from_json.py <JSONæ–‡ä»¶> [é€‰é¡¹]")
        print("\né€‰é¡¹:")
        print("  --output FILE      è¾“å‡ºæ–‡ä»¶å")
        print("  --max-urls N       æœ€å¤§URLæ•°é‡é™åˆ¶")
        print("\nç¤ºä¾‹:")
        print("  # æå–æ‰€æœ‰URL")
        print("  python3 extract_urls_from_json.py author_articles.json")
        print("\n  # é™åˆ¶æå–100ä¸ªURL")
        print("  python3 extract_urls_from_json.py author_articles.json --max-urls 100")
        print("\n  # æŒ‡å®šè¾“å‡ºæ–‡ä»¶")
        print("  python3 extract_urls_from_json.py author_articles.json --output urls.txt")
        print("\nåç»­ä½¿ç”¨:")
        print("  # æ‰¹é‡æŠ“å–æ–‡ç« å†…å®¹")
        print("  python3 zhihu_crawler.py --batch extracted_urls.txt --cookie \"ä½ çš„Cookie\"")
        print("=" * 70)
        sys.exit(1)
    
    # è§£æå‚æ•°
    json_file = sys.argv[1]
    output_file = None
    max_urls = None
    
    i = 2
    while i < len(sys.argv):
        arg = sys.argv[i]
        
        if arg == '--output':
            if i + 1 < len(sys.argv):
                output_file = sys.argv[i + 1]
                i += 2
            else:
                print("âœ— é”™è¯¯: --output éœ€è¦æä¾›æ–‡ä»¶å")
                sys.exit(1)
        elif arg == '--max-urls':
            if i + 1 < len(sys.argv):
                max_urls = int(sys.argv[i + 1])
                i += 2
            else:
                print("âœ— é”™è¯¯: --max-urls éœ€è¦æä¾›æ•°é‡")
                sys.exit(1)
        else:
            print(f"âœ— é”™è¯¯: æœªçŸ¥é€‰é¡¹ {arg}")
            sys.exit(1)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(json_file):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {json_file}")
        sys.exit(1)
    
    try:
        print("=" * 70)
        print("æå–URLåˆ—è¡¨")
        print("=" * 70)
        
        urls = extract_urls_from_json(json_file, output_file, max_urls)
        
        if urls:
            print(f"\nğŸ‰ æå–å®Œæˆï¼")
            print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
            print(f"python3 zhihu_crawler.py --batch {output_file or 'extracted_urls.txt'} --cookie \"ä½ çš„Cookie\"")
        else:
            print("âŒ æœªæå–åˆ°ä»»ä½•URL")
            sys.exit(1)
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {str(e)}")
        sys.exit(1)


if __name__ == '__main__':
    main()