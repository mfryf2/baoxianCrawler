#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çŸ¥ä¹ä½œè€…æ–‡ç« åˆ—è¡¨æŠ“å–å·¥å…·
è·å–æŒ‡å®šä½œè€…çš„æ‰€æœ‰æ–‡ç« æ ‡é¢˜å’ŒURLï¼Œå¹¶å†™å…¥æ•°æ®åº“
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import random
import re
import sys
import os
from datetime import datetime
from urllib.parse import urljoin, urlparse, parse_qs
import pymysql
from pymysql.cursors import DictCursor


class DatabaseManager:
    """æ•°æ®åº“ç®¡ç†ç±»"""
    
    def __init__(self, host='172.105.225.120', user='root', password='lnmp.org#25295', 
                 database='wordpress', port=3306):
        """
        åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        
        Args:
            host: æ•°æ®åº“ä¸»æœº
            user: æ•°æ®åº“ç”¨æˆ·
            password: æ•°æ®åº“å¯†ç 
            database: æ•°æ®åº“å
            port: æ•°æ®åº“ç«¯å£
        """
        self.host = host
        self.user = user
        self.password = password
        self.database = database
        self.port = port
        self.connection = None
    
    def connect(self):
        """å»ºç«‹æ•°æ®åº“è¿æ¥"""
        try:
            self.connection = pymysql.connect(
                host=self.host,
                user=self.user,
                password=self.password,
                database=self.database,
                port=self.port,
                charset='utf8mb4'
            )
            print("âœ“ æ•°æ®åº“è¿æ¥æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
            return False
    
    def disconnect(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.connection:
            self.connection.close()
            print("âœ“ æ•°æ®åº“è¿æ¥å·²å…³é—­")
    
    def insert_article(self, article, author_info):
        """
        æ’å…¥æ–‡ç« åˆ°æ•°æ®åº“
        
        Args:
            article: æ–‡ç« ä¿¡æ¯å­—å…¸
            author_info: ä½œè€…ä¿¡æ¯å­—å…¸
            
        Returns:
            bool: æ’å…¥æ˜¯å¦æˆåŠŸ
        """
        if not self.connection:
            print("âŒ æ•°æ®åº“æœªè¿æ¥")
            return False
        
        try:
            with self.connection.cursor() as cursor:
                # æ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨
                check_sql = "SELECT id FROM baoxianblog WHERE src_url = %s"
                cursor.execute(check_sql, (article['url'],))
                if cursor.fetchone():
                    print(f"âš ï¸  URLå·²å­˜åœ¨ï¼Œè·³è¿‡: {article['url']}")
                    return False
                
                # å‡†å¤‡æ’å…¥æ•°æ®
                insert_sql = """
                    INSERT INTO baoxianblog 
                    (src_url, src_title, src_content, published_user, src_user, 
                     create_time, src_published_time, like_count, collect_count, 
                     from_source, isPublish)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                """
                
                # ä½¿ç”¨æ‘˜è¦ä½œä¸ºå†…å®¹ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç©ºå­—ç¬¦ä¸²
                content = article.get('excerpt', '')[:500] if article.get('excerpt') else ''
                
                # è·å–å‘å¸ƒæ—¶é—´
                published_time = article.get('created_time', '')
                
                values = (
                    article['url'],
                    article['title'],
                    content,
                    author_info['name'],
                    author_info['name'],
                    datetime.now(),
                    published_time if published_time else None,
                    article.get('voteup_count', -1),
                    -1,  # collect_count é»˜è®¤ä¸º-1
                    'zhihu',
                    0  # isPublish é»˜è®¤ä¸º0ï¼ˆæœªå‘å¸ƒï¼‰
                )
                
                cursor.execute(insert_sql, values)
                self.connection.commit()
                print(f"âœ“ æˆåŠŸä¿å­˜: {article['title'][:50]}")
                return True
                
        except Exception as e:
            print(f"âŒ æ’å…¥æ•°æ®åº“å¤±è´¥: {str(e)}")
            self.connection.rollback()
            return False
    
    def insert_articles_batch(self, articles, author_info):
        """
        æ‰¹é‡æ’å…¥æ–‡ç« åˆ°æ•°æ®åº“
        
        Args:
            articles: æ–‡ç« åˆ—è¡¨
            author_info: ä½œè€…ä¿¡æ¯å­—å…¸
            
        Returns:
            dict: åŒ…å«æˆåŠŸå’Œå¤±è´¥è®¡æ•°çš„ç»Ÿè®¡ä¿¡æ¯
        """
        if not self.connection:
            print("âŒ æ•°æ®åº“æœªè¿æ¥")
            return {'success': 0, 'failed': 0, 'skipped': 0}
        
        stats = {'success': 0, 'failed': 0, 'skipped': 0}
        
        try:
            with self.connection.cursor() as cursor:
                for i, article in enumerate(articles, 1):
                    try:
                        # æ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨
                        check_sql = "SELECT id FROM baoxianblog WHERE src_url = %s"
                        cursor.execute(check_sql, (article['url'],))
                        if cursor.fetchone():
                            stats['skipped'] += 1
                            continue
                        
                        # å‡†å¤‡æ’å…¥æ•°æ®
                        insert_sql = """
                            INSERT INTO baoxianblog 
                            (src_url, src_title, src_content, published_user, src_user, 
                             create_time, src_published_time, like_count, collect_count, 
                             from_source, isPublish)
                            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                        """
                        
                        # ä½¿ç”¨æ‘˜è¦ä½œä¸ºå†…å®¹
                        content = article.get('excerpt', '')[:500] if article.get('excerpt') else ''
                        published_time = article.get('created_time', '')
                        
                        values = (
                            article['url'],
                            article['title'],
                            content,
                            author_info['name'],
                            author_info['name'],
                            datetime.now(),
                            published_time if published_time else None,
                            article.get('voteup_count', -1),
                            -1,  # collect_count é»˜è®¤ä¸º-1
                            'zhihu',
                            0
                        )
                        
                        cursor.execute(insert_sql, values)
                        stats['success'] += 1
                        
                        if i % 10 == 0:
                            self.connection.commit()
                            print(f"  å·²å¤„ç† {i}/{len(articles)} ç¯‡æ–‡ç« ...")
                    
                    except Exception as e:
                        stats['failed'] += 1
                        print(f"âš ï¸  å¤„ç†ç¬¬ {i} ç¯‡æ–‡ç« å¤±è´¥: {str(e)}")
                        continue
                
                # æœ€åæäº¤ä¸€æ¬¡
                self.connection.commit()
        
        except Exception as e:
            print(f"âŒ æ‰¹é‡æ’å…¥å¤±è´¥: {str(e)}")
            self.connection.rollback()
        
        return stats


class ZhihuAuthorCrawler:
    def __init__(self, cookie=None, db_manager=None):
        """
        åˆå§‹åŒ–ä½œè€…çˆ¬è™«
        
        Args:
            cookie: å¯é€‰çš„cookieå­—ç¬¦ä¸²ï¼Œç”¨äºç»•è¿‡ç™»å½•é™åˆ¶
            db_manager: å¯é€‰çš„æ•°æ®åº“ç®¡ç†å™¨
        """
        self.user_agents = [
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
        ]
        
        self.base_headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
        }
        
        self.cookie = cookie
        self.session = requests.Session()
        self.db_manager = db_manager
        
        # APIç›¸å…³çš„headers
        self.api_headers = {
            'Accept': '*/*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
            'Referer': 'https://www.zhihu.com/',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
            'x-requested-with': 'fetch',
        }
    
    def _get_headers(self, is_api=False):
        """è·å–è¯·æ±‚å¤´"""
        if is_api:
            headers = self.api_headers.copy()
        else:
            headers = self.base_headers.copy()
        
        headers['User-Agent'] = random.choice(self.user_agents)
        
        if self.cookie:
            headers['Cookie'] = self.cookie
        
        return headers
    
    def _make_request(self, url, max_retries=5, is_api=False):
        """
        å‘é€HTTPè¯·æ±‚ï¼Œå¸¦æ™ºèƒ½é‡è¯•
        
        Args:
            url: ç›®æ ‡URL
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            is_api: æ˜¯å¦æ˜¯APIè¯·æ±‚
            
        Returns:
            requests.Response: å“åº”å¯¹è±¡
        """
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    wait_time = (2 ** attempt) + random.uniform(0, 1)
                    print(f"âš ï¸  ç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                
                headers = self._get_headers(is_api=is_api)
                
                response = self.session.get(
                    url,
                    headers=headers,
                    timeout=20,
                    allow_redirects=True
                )
                
                if response.status_code == 403:
                    if attempt < max_retries - 1:
                        print(f"âš ï¸  é‡åˆ°403é”™è¯¯ï¼Œå°è¯•æ›´æ¢User-Agenté‡è¯• ({attempt + 1}/{max_retries})")
                        continue
                    else:
                        raise Exception("403 Forbidden - éœ€è¦æä¾›æœ‰æ•ˆçš„Cookie")
                
                if response.status_code == 429:
                    wait_time = 5 + random.uniform(0, 5)
                    print(f"âš ï¸  è¯·æ±‚è¿‡å¿«ï¼Œç­‰å¾… {wait_time:.1f} ç§’...")
                    time.sleep(wait_time)
                    continue
                
                response.raise_for_status()
                
                return response
                
            except requests.Timeout:
                if attempt < max_retries - 1:
                    print(f"âš ï¸  è¯·æ±‚è¶…æ—¶ï¼Œé‡è¯•ä¸­ ({attempt + 1}/{max_retries})")
                    continue
                else:
                    raise Exception("è¯·æ±‚è¶…æ—¶")
            
            except requests.RequestException as e:
                if attempt < max_retries - 1:
                    print(f"âš ï¸  è¯·æ±‚å¤±è´¥: {str(e)}, é‡è¯•ä¸­ ({attempt + 1}/{max_retries})")
                    continue
                else:
                    raise Exception(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
        
        raise Exception("è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
    
    def _extract_author_info(self, author_url):
        """
        ä»ä½œè€…é¡µé¢URLæå–ä½œè€…ä¿¡æ¯
        
        Args:
            author_url: ä½œè€…é¡µé¢URL
            
        Returns:
            dict: åŒ…å«ä½œè€…ä¿¡æ¯çš„å­—å…¸
        """
        print(f"æ­£åœ¨è·å–ä½œè€…ä¿¡æ¯: {author_url}")
        
        response = self._make_request(author_url)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æå–ä½œè€…åç§°
        author_name = "æœªçŸ¥ä½œè€…"
        name_selectors = [
            ('h1', {'class': 'ProfileHeader-title'}),
            ('span', {'class': 'ProfileHeader-name'}),
            ('h1', {}),
        ]
        
        for tag, attrs in name_selectors:
            name_element = soup.find(tag, attrs)
            if name_element:
                author_name = name_element.get_text().strip()
                break
        
        # æå–ä½œè€…IDï¼ˆä»URLä¸­ï¼‰
        author_id = ""
        if '/org/' in author_url:
            # æœºæ„è´¦å·
            match = re.search(r'/org/([^/]+)', author_url)
            if match:
                author_id = match.group(1)
        else:
            # ä¸ªäººè´¦å·
            match = re.search(r'/people/([^/]+)', author_url)
            if match:
                author_id = match.group(1)
        
        # å°è¯•ä»é¡µé¢ä¸­æå–æ–‡ç« æ€»æ•°
        total_posts = 0
        post_count_patterns = [
            r'(\d+)\s*ç¯‡æ–‡ç« ',
            r'(\d+)\s*ä¸ªå›ç­”',
            r'å‘è¡¨äº†\s*(\d+)\s*ç¯‡æ–‡ç« ',
        ]
        
        page_text = soup.get_text()
        for pattern in post_count_patterns:
            match = re.search(pattern, page_text)
            if match:
                total_posts = int(match.group(1))
                break
        
        return {
            'name': author_name,
            'id': author_id,
            'url': author_url,
            'total_posts': total_posts
        }
    
    def _get_api_url(self, author_url):
        """
        æ ¹æ®ä½œè€…é¡µé¢URLæ„å»ºAPI URL
        
        Args:
            author_url: ä½œè€…é¡µé¢URL
            
        Returns:
            str: API URL
        """
        if '/org/' in author_url:
            # æœºæ„è´¦å· - å°è¯•å¤šç§å¯èƒ½çš„APIç«¯ç‚¹
            match = re.search(r'/org/([^/]+)', author_url)
            if match:
                org_id = match.group(1)
                # è¿”å›å¤šä¸ªå¯èƒ½çš„API URLä¾›å°è¯•
                return [
                    f"https://www.zhihu.com/api/v4/members/{org_id}/articles",
                    f"https://zhuanlan.zhihu.com/api/columns/{org_id}/articles",
                    f"https://www.zhihu.com/api/v4/org/{org_id}/articles"
                ]
        else:
            # ä¸ªäººè´¦å·
            match = re.search(r'/people/([^/]+)', author_url)
            if match:
                user_id = match.group(1)
                return [
                    f"https://www.zhihu.com/api/v4/members/{user_id}/articles",
                    f"https://zhuanlan.zhihu.com/api/columns/{user_id}/articles"
                ]
        
        raise Exception("æ— æ³•è¯†åˆ«çš„ä½œè€…URLæ ¼å¼")
    
    def fetch_author_articles(self, author_url, max_articles=None, delay=1.0):
        """
        è·å–ä½œè€…çš„æ‰€æœ‰æ–‡ç« 
        
        Args:
            author_url: ä½œè€…é¡µé¢URL
            max_articles: æœ€å¤§æ–‡ç« æ•°é‡é™åˆ¶ï¼ŒNoneè¡¨ç¤ºè·å–æ‰€æœ‰
            delay: è¯·æ±‚é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            dict: åŒ…å«ä½œè€…ä¿¡æ¯å’Œæ–‡ç« åˆ—è¡¨çš„å­—å…¸
        """
        # è·å–ä½œè€…ä¿¡æ¯
        author_info = self._extract_author_info(author_url)
        print(f"âœ“ ä½œè€…: {author_info['name']}")
        if author_info['total_posts'] > 0:
            print(f"âœ“ é¢„è®¡æ–‡ç« æ•°: {author_info['total_posts']}")
        
        # æ„å»ºAPI URLåˆ—è¡¨
        try:
            api_urls = self._get_api_url(author_url)
        except Exception as e:
            print(f"âŒ {str(e)}")
            return None
        
        articles = []
        offset = 0
        limit = 20  # æ¯é¡µæ–‡ç« æ•°
        page = 1
        working_api_url = None
        
        print(f"\nå¼€å§‹è·å–æ–‡ç« åˆ—è¡¨...")
        
        # é¦–å…ˆæ‰¾åˆ°å¯ç”¨çš„APIç«¯ç‚¹
        if not working_api_url:
            print("æ­£åœ¨å¯»æ‰¾å¯ç”¨çš„APIç«¯ç‚¹...")
            for api_url in api_urls:
                try:
                    params = {
                        'limit': 5,  # æµ‹è¯•æ—¶åªè·å–å°‘é‡æ•°æ®
                        'offset': 0,
                        'sort_by': 'created'
                    }
                    test_url = api_url + '?' + '&'.join([f"{k}={v}" for k, v in params.items()])
                    print(f"  å°è¯•: {api_url}")
                    
                    response = self._make_request(test_url, is_api=True)
                    data = response.json()
                    
                    if 'data' in data and isinstance(data['data'], list):
                        working_api_url = api_url
                        print(f"  âœ… æ‰¾åˆ°å¯ç”¨API: {api_url}")
                        break
                    else:
                        print(f"  âŒ APIå“åº”æ ¼å¼ä¸æ­£ç¡®")
                        
                except Exception as e:
                    print(f"  âŒ APIä¸å¯ç”¨: {str(e)}")
                    continue
            
            if not working_api_url:
                print("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„APIç«¯ç‚¹ï¼Œå°è¯•ä»é¡µé¢HTMLè§£æ...")
                return self._parse_from_html(author_url, author_info, max_articles)
        
        while True:
            try:
                # æ„å»ºåˆ†é¡µAPI URL
                params = {
                    'limit': limit,
                    'offset': offset,
                    'sort_by': 'created'
                }
                
                current_url = working_api_url + '?' + '&'.join([f"{k}={v}" for k, v in params.items()])
                
                print(f"æ­£åœ¨è·å–ç¬¬ {page} é¡µ (å·²è·å– {len(articles)} ç¯‡)...")
                
                response = self._make_request(current_url, is_api=True)
                
                try:
                    data = response.json()
                except json.JSONDecodeError:
                    print("âŒ APIå“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼")
                    break
                
                # æ£€æŸ¥APIå“åº”æ ¼å¼
                if 'data' not in data:
                    print("âŒ APIå“åº”æ ¼å¼å¼‚å¸¸")
                    break
                
                page_articles = data['data']
                
                if not page_articles:
                    print("âœ“ å·²è·å–æ‰€æœ‰æ–‡ç« ")
                    break
                
                # å¤„ç†å½“å‰é¡µçš„æ–‡ç« 
                for article in page_articles:
                    try:
                        article_info = {
                            'title': article.get('title', 'æ— æ ‡é¢˜'),
                            'url': article.get('url', ''),
                            'id': article.get('id', ''),
                            'created_time': article.get('created_time', ''),
                            'updated_time': article.get('updated_time', ''),
                            'excerpt': article.get('excerpt', ''),
                            'voteup_count': article.get('voteup_count', 0),
                            'comment_count': article.get('comment_count', 0),
                        }
                        
                        # æ ¼å¼åŒ–æ—¶é—´
                        if article_info['created_time']:
                            try:
                                timestamp = int(article_info['created_time'])
                                article_info['created_time'] = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')
                            except:
                                pass
                        
                        articles.append(article_info)
                        
                        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ•°é‡é™åˆ¶
                        if max_articles and len(articles) >= max_articles:
                            print(f"âœ“ å·²è¾¾åˆ°æœ€å¤§æ–‡ç« æ•°é‡é™åˆ¶: {max_articles}")
                            break
                            
                    except Exception as e:
                        print(f"âš ï¸  å¤„ç†æ–‡ç« ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
                        continue
                
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šæ–‡ç« 
                if len(page_articles) < limit:
                    print("âœ“ å·²è·å–æ‰€æœ‰æ–‡ç« ")
                    break
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ•°é‡é™åˆ¶
                if max_articles and len(articles) >= max_articles:
                    break
                
                # å‡†å¤‡ä¸‹ä¸€é¡µ
                offset += limit
                page += 1
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                if delay > 0:
                    time.sleep(delay)
                
            except KeyboardInterrupt:
                print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜å·²è·å–çš„æ–‡ç« ...")
                break
            except Exception as e:
                print(f"âŒ è·å–ç¬¬ {page} é¡µæ—¶å‡ºé”™: {str(e)}")
                break
        
        result = {
            'author': author_info,
            'articles': articles,
            'total_fetched': len(articles),
            'fetch_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        print(f"\nâœ… è·å–å®Œæˆï¼å…±è·å– {len(articles)} ç¯‡æ–‡ç« ")
        
        # å¦‚æœæä¾›äº†æ•°æ®åº“ç®¡ç†å™¨ï¼Œå°†æ•°æ®ä¿å­˜åˆ°æ•°æ®åº“
        if self.db_manager and articles:
            print("\n" + "=" * 80)
            print("å¼€å§‹å°†æ•°æ®ä¿å­˜åˆ°æ•°æ®åº“...")
            print("=" * 80)
            
            stats = self.db_manager.insert_articles_batch(articles, author_info)
            result['db_stats'] = stats
            
            print(f"\næ•°æ®åº“ä¿å­˜ç»Ÿè®¡:")
            print(f"  æˆåŠŸ: {stats['success']} ç¯‡")
            print(f"  å¤±è´¥: {stats['failed']} ç¯‡")
            print(f"  è·³è¿‡: {stats['skipped']} ç¯‡ (å·²å­˜åœ¨)")
        
        return result
    
    def _parse_from_html(self, author_url, author_info, max_articles=None):
        """
        ä»é¡µé¢HTMLè§£ææ–‡ç« åˆ—è¡¨ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        
        Args:
            author_url: ä½œè€…é¡µé¢URL
            author_info: ä½œè€…ä¿¡æ¯
            max_articles: æœ€å¤§æ–‡ç« æ•°é‡é™åˆ¶
            
        Returns:
            dict: åŒ…å«ä½œè€…ä¿¡æ¯å’Œæ–‡ç« åˆ—è¡¨çš„å­—å…¸
        """
        print("ä½¿ç”¨HTMLè§£ææ–¹æ³•è·å–æ–‡ç« åˆ—è¡¨...")
        
        articles = []
        page = 1
        
        try:
            # è·å–ç¬¬ä¸€é¡µ
            response = self._make_request(author_url)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–‡ç« é“¾æ¥çš„å¤šç§é€‰æ‹©å™¨
            article_selectors = [
                'a[href*="/p/"]',  # ä¸“æ æ–‡ç« 
                'a[href*="/answer/"]',  # å›ç­”
                '.ContentItem-title a',  # å†…å®¹é¡¹æ ‡é¢˜
                '.Post-Title a',  # æ–‡ç« æ ‡é¢˜
                '.ArticleItem-title a',  # æ–‡ç« é¡¹æ ‡é¢˜
            ]
            
            found_articles = []
            
            for selector in article_selectors:
                elements = soup.select(selector)
                if elements:
                    print(f"  æ‰¾åˆ° {len(elements)} ä¸ªé“¾æ¥ (é€‰æ‹©å™¨: {selector})")
                    found_articles.extend(elements)
            
            # å»é‡å¹¶å¤„ç†æ–‡ç« é“¾æ¥
            seen_urls = set()
            
            for element in found_articles:
                try:
                    href = element.get('href', '')
                    if not href:
                        continue
                    
                    # æ„å»ºå®Œæ•´URL
                    if href.startswith('/'):
                        full_url = 'https://www.zhihu.com' + href
                    elif href.startswith('http'):
                        full_url = href
                    else:
                        continue
                    
                    # åªå¤„ç†æ–‡ç« URLï¼Œè·³è¿‡å…¶ä»–ç±»å‹
                    if '/p/' not in full_url and '/answer/' not in full_url:
                        continue
                    
                    if full_url in seen_urls:
                        continue
                    
                    seen_urls.add(full_url)
                    
                    # æå–æ ‡é¢˜
                    title = element.get_text().strip()
                    if not title or len(title) < 3:
                        # å°è¯•ä»çˆ¶å…ƒç´ è·å–æ ‡é¢˜
                        parent = element.parent
                        if parent:
                            title = parent.get_text().strip()
                    
                    if title and len(title) >= 3:
                        article_info = {
                            'title': title,
                            'url': full_url,
                            'id': self._extract_id_from_url(full_url),
                            'created_time': '',
                            'updated_time': '',
                            'excerpt': '',
                            'voteup_count': 0,
                            'comment_count': 0,
                        }
                        
                        articles.append(article_info)
                        
                        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ•°é‡é™åˆ¶
                        if max_articles and len(articles) >= max_articles:
                            break
                            
                except Exception as e:
                    print(f"âš ï¸  å¤„ç†æ–‡ç« é“¾æ¥æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            print(f"âœ… ä»HTMLè§£æè·å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
            
            # å°è¯•è·å–æ›´å¤šé¡µé¢ï¼ˆå¦‚æœæœ‰åˆ†é¡µï¼‰
            if len(articles) < (max_articles or 100):
                print("å°è¯•è·å–æ›´å¤šé¡µé¢...")
                # è¿™é‡Œå¯ä»¥æ·»åŠ åˆ†é¡µé€»è¾‘ï¼Œä½†çŸ¥ä¹çš„åˆ†é¡µé€šå¸¸æ˜¯åŠ¨æ€åŠ è½½çš„
                # æ‰€ä»¥HTMLè§£ææ–¹æ³•ä¸»è¦è·å–ç¬¬ä¸€é¡µçš„å†…å®¹
            
        except Exception as e:
            print(f"âŒ HTMLè§£æå¤±è´¥: {str(e)}")
        
        result = {
            'author': author_info,
            'articles': articles,
            'total_fetched': len(articles),
            'fetch_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        return result
    
    def _extract_id_from_url(self, url):
        """ä»URLä¸­æå–æ–‡ç« ID"""
        try:
            if '/p/' in url:
                match = re.search(r'/p/(\d+)', url)
                if match:
                    return match.group(1)
            elif '/answer/' in url:
                match = re.search(r'/answer/(\d+)', url)
                if match:
                    return match.group(1)
        except:
            pass
        return ''
    
    def save_to_json(self, data, output_file=None):
        """
        ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶
        
        Args:
            data: è¦ä¿å­˜çš„æ•°æ®
            output_file: è¾“å‡ºæ–‡ä»¶å
            
        Returns:
            str: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if not output_file:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            author_name = data['author']['name']
            safe_name = "".join(c for c in author_name if c.isalnum() or c in (' ', '-', '_')).strip()
            safe_name = safe_name[:30]  # é™åˆ¶æ–‡ä»¶åé•¿åº¦
            output_file = f"{safe_name}_articles_{timestamp}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
        return output_file
    
    def save_to_txt(self, data, output_file=None):
        """
        ä¿å­˜æ•°æ®åˆ°æ–‡æœ¬æ–‡ä»¶
        
        Args:
            data: è¦ä¿å­˜çš„æ•°æ®
            output_file: è¾“å‡ºæ–‡ä»¶å
            
        Returns:
            str: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if not output_file:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            author_name = data['author']['name']
            safe_name = "".join(c for c in author_name if c.isalnum() or c in (' ', '-', '_')).strip()
            safe_name = safe_name[:30]  # é™åˆ¶æ–‡ä»¶åé•¿åº¦
            output_file = f"{safe_name}_articles_{timestamp}.txt"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            # å†™å…¥ä½œè€…ä¿¡æ¯
            f.write("=" * 80 + "\n")
            f.write(f"ä½œè€…ä¿¡æ¯\n")
            f.write("=" * 80 + "\n")
            f.write(f"ä½œè€…åç§°: {data['author']['name']}\n")
            f.write(f"ä½œè€…ID: {data['author']['id']}\n")
            f.write(f"ä½œè€…é¡µé¢: {data['author']['url']}\n")
            f.write(f"è·å–æ—¶é—´: {data['fetch_time']}\n")
            f.write(f"æ–‡ç« æ€»æ•°: {data['total_fetched']}\n")
            f.write("\n")
            
            # å†™å…¥æ–‡ç« åˆ—è¡¨
            f.write("=" * 80 + "\n")
            f.write(f"æ–‡ç« åˆ—è¡¨ (å…± {len(data['articles'])} ç¯‡)\n")
            f.write("=" * 80 + "\n")
            
            for i, article in enumerate(data['articles'], 1):
                f.write(f"\n{i:4d}. {article['title']}\n")
                f.write(f"      URL: {article['url']}\n")
                if article['created_time']:
                    f.write(f"      å‘å¸ƒæ—¶é—´: {article['created_time']}\n")
                if article['voteup_count'] > 0:
                    f.write(f"      ç‚¹èµæ•°: {article['voteup_count']}\n")
                if article['comment_count'] > 0:
                    f.write(f"      è¯„è®ºæ•°: {article['comment_count']}\n")
                if article['excerpt']:
                    excerpt = article['excerpt'][:100] + "..." if len(article['excerpt']) > 100 else article['excerpt']
                    f.write(f"      æ‘˜è¦: {excerpt}\n")
        
        print(f"âœ“ æ–‡æœ¬æ–‡ä»¶å·²ä¿å­˜åˆ°: {output_file}")
        return output_file
    
    def save_to_csv(self, data, output_file=None):
        """
        ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶
        
        Args:
            data: è¦ä¿å­˜çš„æ•°æ®
            output_file: è¾“å‡ºæ–‡ä»¶å
            
        Returns:
            str: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        import csv
        
        if not output_file:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            author_name = data['author']['name']
            safe_name = "".join(c for c in author_name if c.isalnum() or c in (' ', '-', '_')).strip()
            safe_name = safe_name[:30]  # é™åˆ¶æ–‡ä»¶åé•¿åº¦
            output_file = f"{safe_name}_articles_{timestamp}.csv"
        
        with open(output_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            
            # å†™å…¥è¡¨å¤´
            writer.writerow([
                'åºå·', 'æ ‡é¢˜', 'URL', 'æ–‡ç« ID', 'å‘å¸ƒæ—¶é—´', 
                'ç‚¹èµæ•°', 'è¯„è®ºæ•°', 'æ‘˜è¦'
            ])
            
            # å†™å…¥æ–‡ç« æ•°æ®
            for i, article in enumerate(data['articles'], 1):
                writer.writerow([
                    i,
                    article['title'],
                    article['url'],
                    article['id'],
                    article['created_time'],
                    article['voteup_count'],
                    article['comment_count'],
                    article['excerpt']
                ])
        
        print(f"âœ“ CSVæ–‡ä»¶å·²ä¿å­˜åˆ°: {output_file}")
        return output_file


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("=" * 80)
        print("çŸ¥ä¹ä½œè€…æ–‡ç« åˆ—è¡¨æŠ“å–å·¥å…·")
        print("=" * 80)
        print("\nä½¿ç”¨æ–¹æ³•:")
        print("  python3 zhihu_author_crawler.py <ä½œè€…URL> [é€‰é¡¹]")
        print("\né€‰é¡¹:")
        print("  --max-articles N    æœ€å¤§æ–‡ç« æ•°é‡é™åˆ¶")
        print("  --delay N          è¯·æ±‚é—´éš”æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤1.0ï¼‰")
        print("  --format FORMAT    è¾“å‡ºæ ¼å¼: json, txt, csv, allï¼ˆé»˜è®¤allï¼‰")
        print("  --output FILE      è¾“å‡ºæ–‡ä»¶åå‰ç¼€")
        print("  --cookie COOKIE    Cookieå­—ç¬¦ä¸²")
        print("\nç¤ºä¾‹:")
        print("  # è·å–æ‰€æœ‰æ–‡ç« ")
        print("  python3 zhihu_author_crawler.py https://www.zhihu.com/org/nai-ba-bao-25/posts")
        print("\n  # é™åˆ¶è·å–100ç¯‡æ–‡ç« ")
        print("  python3 zhihu_author_crawler.py https://www.zhihu.com/org/nai-ba-bao-25/posts --max-articles 100")
        print("\n  # åªä¿å­˜ä¸ºJSONæ ¼å¼")
        print("  python3 zhihu_author_crawler.py https://www.zhihu.com/org/nai-ba-bao-25/posts --format json")
        print("\n  # ä½¿ç”¨Cookie")
        print("  python3 zhihu_author_crawler.py https://www.zhihu.com/org/nai-ba-bao-25/posts --cookie 'xxx'")
        print("\næ”¯æŒçš„URLæ ¼å¼:")
        print("  - æœºæ„è´¦å·: https://www.zhihu.com/org/æœºæ„ID/posts")
        print("  - ä¸ªäººè´¦å·: https://www.zhihu.com/people/ç”¨æˆ·ID/posts")
        print("=" * 80)
        sys.exit(1)
    
    # è§£æå‚æ•°
    author_url = sys.argv[1]
    max_articles = None
    delay = 1.0
    output_format = 'all'
    output_prefix = None
    cookie = None
    
    i = 2
    while i < len(sys.argv):
        arg = sys.argv[i]
        
        if arg == '--max-articles':
            if i + 1 < len(sys.argv):
                max_articles = int(sys.argv[i + 1])
                i += 2
            else:
                print("âœ— é”™è¯¯: --max-articles éœ€è¦æä¾›æ•°é‡")
                sys.exit(1)
        elif arg == '--delay':
            if i + 1 < len(sys.argv):
                delay = float(sys.argv[i + 1])
                i += 2
            else:
                print("âœ— é”™è¯¯: --delay éœ€è¦æä¾›æ—¶é—´")
                sys.exit(1)
        elif arg == '--format':
            if i + 1 < len(sys.argv):
                output_format = sys.argv[i + 1]
                if output_format not in ['json', 'txt', 'csv', 'all']:
                    print("âœ— é”™è¯¯: æ ¼å¼å¿…é¡»æ˜¯ json, txt, csv æˆ– all")
                    sys.exit(1)
                i += 2
            else:
                print("âœ— é”™è¯¯: --format éœ€è¦æä¾›æ ¼å¼")
                sys.exit(1)
        elif arg == '--output':
            if i + 1 < len(sys.argv):
                output_prefix = sys.argv[i + 1]
                i += 2
            else:
                print("âœ— é”™è¯¯: --output éœ€è¦æä¾›æ–‡ä»¶åå‰ç¼€")
                sys.exit(1)
        elif arg == '--cookie':
            if i + 1 < len(sys.argv):
                cookie = sys.argv[i + 1]
                i += 2
            else:
                print("âœ— é”™è¯¯: --cookie éœ€è¦æä¾›Cookieå€¼")
                sys.exit(1)
        else:
            print(f"âœ— é”™è¯¯: æœªçŸ¥é€‰é¡¹ {arg}")
            sys.exit(1)
    
    # æ£€æŸ¥Cookie
    if not cookie and os.path.exists('cookie.txt'):
        with open('cookie.txt', 'r', encoding='utf-8') as f:
            cookie = f.read().strip()
        print("âœ“ å·²ä» cookie.txt åŠ è½½Cookie")
    
    if not cookie:
        print("âš ï¸  è­¦å‘Š: æœªæä¾›Cookieï¼Œå¯èƒ½ä¼šé‡åˆ°è®¿é—®é™åˆ¶")
        print("å»ºè®®å…ˆè¿è¡Œ: python3 get_cookie_helper.py")
    
    # åˆ›å»ºæ•°æ®åº“ç®¡ç†å™¨å®ä¾‹
    db_manager = DatabaseManager()
    if not db_manager.connect():
        print("âŒ æ— æ³•è¿æ¥æ•°æ®åº“ï¼Œé€€å‡ºç¨‹åºã€‚")
        sys.exit(1)
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = ZhihuAuthorCrawler(cookie=cookie, db_manager=db_manager)
    
    try:
        print("=" * 80)
        print("å¼€å§‹æŠ“å–ä½œè€…æ–‡ç« åˆ—è¡¨")
        print("=" * 80)
        
        # è·å–æ–‡ç« æ•°æ®
        data = crawler.fetch_author_articles(
            author_url, 
            max_articles=max_articles, 
            delay=delay
        )
        
        if not data:
            print("âŒ è·å–å¤±è´¥")
            sys.exit(1)
        
        print("\n" + "=" * 80)
        print("ä¿å­˜æ•°æ®")
        print("=" * 80)
        
        # ä¿å­˜æ•°æ®
        if output_format == 'all':
            crawler.save_to_json(data, f"{output_prefix}.json" if output_prefix else None)
            crawler.save_to_txt(data, f"{output_prefix}.txt" if output_prefix else None)
            crawler.save_to_csv(data, f"{output_prefix}.csv" if output_prefix else None)
        elif output_format == 'json':
            crawler.save_to_json(data, f"{output_prefix}.json" if output_prefix else None)
        elif output_format == 'txt':
            crawler.save_to_txt(data, f"{output_prefix}.txt" if output_prefix else None)
        elif output_format == 'csv':
            crawler.save_to_csv(data, f"{output_prefix}.csv" if output_prefix else None)
        
        print(f"\nğŸ‰ ä»»åŠ¡å®Œæˆï¼å…±è·å– {data['total_fetched']} ç¯‡æ–‡ç« ")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {str(e)}")
        if '403' in str(e) or 'Cookie' in str(e):
            print("\nğŸ’¡ æç¤º: éœ€è¦æä¾›æœ‰æ•ˆçš„Cookie")
            print("   è¿è¡Œ 'python3 get_cookie_helper.py' æŸ¥çœ‹å¦‚ä½•è·å–Cookie")
        sys.exit(1)
    finally:
        db_manager.disconnect()


if __name__ == '__main__':
    main()